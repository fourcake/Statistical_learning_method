# 第六章 逻辑斯蒂回归与最大熵模型

## 本章概要

1. 设X是连续随机变量，X服从逻辑斯谛分布是指X具有下列分布函数和密度函数
   $$
   F(x)=P(X≤x)=\frac{1}{1+e^{-\frac{(x-μ)}{γ}}}
   $$

   $$
   f(x)=\frac{e^-\frac{(x-μ)}{γ}}{γ(1+{e^-\frac{(x-μ)}{γ}}{})^2}
   $$

   其中，μ为位置参数，γ>0为形状参数，下图为密度函数和分布函数的图像

   <img src="images/6.%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/image-20211109154632715.png" alt="image-20211109154632715" style="zoom:80%;" />

2. 二项逻辑斯蒂回归模型如下，模型参数估计常用极大似然估计

   <img src="images/6.%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/image-20211109154651601.png" alt="image-20211109154651601" style="zoom:75%;" />

   

3. 最大熵原理是概率模型学习的⼀个准则。假设分类模型是⼀个条件概率分布P(Y|X)，最大熵模型表示的是对于给
   定的输入X，以条件概率P(Y|X)输出Y

   定义在条件概率分布P(Y|X)上的条件熵为

   <img src="images/6.%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/image-20211109154716966.png" alt="image-20211109154716966" style="zoom:80%;" />

   则熵最大的模型称为最大熵模型

4. **逻辑斯蒂回归模型**
   $$
   f(x)=\frac{1}{1+e^-z};z=w_0x_0+w_1x_1+\cdots+w_nx_n
   $$
   常使用梯度上升算法找到最佳参数（梯度上升用来找函数最大值，梯度下降用来求函数最小值）

## 算法实现

1. 处理数据文件

   ```python
   def loadDataSet():
       dataMat=[]
       labelMat=[]
       fr=open('LogistictestSet.txt')
       for line in fr.readlines():
           lineArr=line.strip().split()
           dataMat.append([1.0,float(lineArr[0]),float(lineArr[1])])
           labelMat.append(int(lineArr[2]))
       return dataMat,labelMat
   ```

2. 设置sigmod函数

   ```python
   def sigmoid(x):
       if x>=0:      #对sigmoid函数的优化，避免了出现极大的数据溢出
           return 1.0/(1+np.exp(-x))
       else:
           return np.exp(x)/(1+np.exp(x))
   ```

3. 简单梯度上升算法

   ```python
   def gradAscent(dataMatIn, classLabels):#梯度上升
       dataMatrix =np.mat(dataMatIn)             #转换成numpy矩阵
       labelMat = np.mat(classLabels).transpose()#转成列向量
       m,n = np.shape(dataMatrix)
       alpha = 0.001
       maxCycles = 500
       weights =np.ones((n,1))
       for k in range(maxCycles):              #迭代500次
           h = sigmoid(dataMatrix*weights)     #矩阵相乘
           error = (labelMat - h)              #向量相减
           weights = weights + alpha * dataMatrix.transpose()* error #矩阵乘法
       return weights
   ```

   对LogistictestSet中的数据进行逻辑回归分类得到

   <img src="images/6.%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/image-20211109154741141.png" alt="image-20211109154741141" style="zoom:80%;" />

   该算法在每次更新时都要遍历整个数据集，当数据量有成千上万个时，复杂度太高，一种改进的方法是一次仅用一个样本点来更新系数，随机梯度上升法

4.  随机梯度上升算法

   ```python
   def stocGradAscent1(dataMatrix, classLabels, numIter=150):
       m,n =np.shape(dataMatrix)
       weights =np.ones(n)   #系数全部初始化为1
       for j in range(numIter):
           dataIndex =list(range(m))
           for i in range(m):
               alpha = 4/(1.0+j+i)+0.0001    #apha随迭代次数变化
               randIndex = int(np.random.uniform(0,len(dataIndex)))#go to 0 because of the constant
               h = sigmoid(sum(dataMatrix[randIndex]*weights))
               error = classLabels[randIndex] - h
               weights = weights + alpha * error * dataMatrix[randIndex]
               del(dataIndex[randIndex])
       return weights
   ```

5.  应用到实例

   ```python
   def classifyVector(inX, weights):
       prob = sigmoid(sum(inX*weights))
       if prob > 0.5: return 1.0
       else: return 0.0
   ```

   ```python
   def colicTest():
       frTrain = open('horseColicTraining.txt')
       frTest = open('horseColicTest.txt')
       trainingSet = []
       trainingLabels = []
       #将train文件数据处理到trainingSet和trainingLabels文件
       for line in frTrain.readlines():
           currLine = line.strip().split('\t')
           lineArr =[]
           for i in range(21):
               lineArr.append(float(currLine[i]))
           trainingSet.append(lineArr)
           trainingLabels.append(float(currLine[21]))
       #训练weights
       trainWeights = stocGradAscent1(np.array(trainingSet), trainingLabels, 1000)
       errorCount = 0
       numTestVec = 0.0
       for line in frTest.readlines():
           numTestVec += 1.0
           currLine = line.strip().split('\t')
           lineArr =[]
           for i in range(21):
               lineArr.append(float(currLine[i]))
           if int(classifyVector(np.array(lineArr), trainWeights))!= int(currLine[21]):
               errorCount += 1
       errorRate = (float(errorCount)/numTestVec)
       print ("the error rate of this test is: %f" % errorRate)
       return errorRate
   ```

6. 得到结果

   ```python
   def multiTest():
       numTests = 10
       errorSum=0.0
       for k in range(numTests):
           errorSum += colicTest()
       print ("after %d iterations the average error rate is: %f" % (numTests, errorSum/float(numTests)))
   ```

   ![image-20211109154753232](images/6.%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/image-20211109154753232.png)

