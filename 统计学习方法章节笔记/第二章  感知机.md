# 第二章  感知机

## 本章概要

1. 感知机是⼆类分类的线性分类模型，输⼊为实例的特征向量，输出为实例的类别，属于判别模型，是神经网络和支持向量机的基础

2. 感知机模型：$f(x)=sign(w \cdot x + b)$，$sign$​是符号函数。感知机模型的假设空间是定义在特征空间中的所有线性分类模型或线性分类器，即函数集合$\{f|f(x)=w \cdot x + b\}$

3. 假设训练数据集线性可分（存在某个**超平面(自由度比空间维度小1)**能够将数据集的正负实例点完全正确的划分到超平面的两侧），感知机学习目的是为了找出这样的超平面，为确定参数$w,b$，需要确定⼀个学习策略，即定义（经验）损失函数并将损失函数极小化

4. 感知机学习策略分析：

   1. 误分类点的总数：这种损失函数不是参数$w,b$的连续可导函数，不易优化
   2. 误分类点到超平面S的总距离

   任一点$x_0$到超平面S的距离：$\frac{1}{||w||}|w^T \cdot x_0 + b$，$||w||$是w的$L_2$范数：推导如下：对于超平面$w^T \cdot x + b =0$，假设$x^{'}$是超平面任意一点，对于空间上任意一点$x$，到平面 A  的距离 H，等于 x 到超平面的法向量长度：$H=|\frac{w^T}{||w||}(x-x^{'})|$，又因为$w^{T}x^{'}=-b$，所以距离为
   $$
   H=\frac{w^T \cdot x+b}{||w||}
   $$
   将距离公式中分子的绝对值去掉, 让它可以为正为负. 那么, 它的值正得越大, 代表点在平面的正向且与平面的距离越远. 反之, 它的值负得越大, 代表点在平面的反向且与平面的距离越远。

   而对于误分类的数据（$x_i,y_i$）来说，当$w^T·x_i +b>0$时，$y_i ＝-1$（**本该为1，但是误分类所以为-1**），反之亦然，因此，误分类点$x_i$到超平⾯S的距离是
   $$
   H=-\frac{1}{||w||}y_i(w^T\cdot x_i+b)
   $$
   假设超平面S的误分类点集合为M，那么所有误分类点到S的总距离为
   $$
   H=-\frac{1}{||w||}\sum_{x_i\in M} y_i(w^T\cdot x_i+b)
   $$
   不考虑$\frac{1}{||w||}$，即得到感知机学习的损失函数
   $$
   L(w,b)=-\sum_{x_i\in M} y_i(w^T\cdot x_i+b)
   $$
   显然，损失函数是非负的。如果没有误分类点，损失函数值是0。

5. 感知机学习的具体算法包括**原始形式**和**对偶形式**。感知机学习算法是误分类驱动的，具体采用随机梯度下降法，损失函数的梯度即为
   $$
   \bigtriangledown_wL(w,b)=-\sum_{x_i \in M}y_ix_i
   $$

   $$
   \bigtriangledown_bL(w,b)=-\sum_{x_i \in M}y_i
   $$

   更新过程,$\eta$为学习步长（学习率），一直更新到损失函数减小到0
   $$
   w\leftarrow w+\eta y_ix_i
   $$

   $$
   b\leftarrow b+\eta y_i
   $$

6. 感知机学习算法存在许多解，这些解既依赖于初值的选择，也依赖于迭代过程中误分类点的选择顺序。为了得到唯一的超平面，需要对分离超平面增加约束条件，这就是第7章将要讲述的线性支持向量机的想法

## 感知机学习算法的原始形式

输入：线性可分训练数据集$T＝{(x_1 ，y_1 ),(x_2 ,y_2 ),…,(x_N ,y_N )}$，其中$y_i\in\{-1,+1\}$，i＝1,2,…,N；学习率 $0<\eta\leqq 1$

输出：$w，b$

感知机模型：$f(x)＝sign(w^T\cdot x+b)$

损失函数（策略）：$\min_{w,b} L(w,b)=-\sum_{x_i\in M} y_i(w^T\cdot x_i+b)$

步骤：

（1）选取初值$w_0,b_0$
（2）在训练集中选取数据$(x_i ，y_i )$（注意$x_i$表数据，$y_i$为标签）
（3）如果$y_i (w^T \cdot x_i +b)≤0$，即该点为误分类点
（4）转至（2），直至训练集中没有误分类点

例题见课本

## 感知机学习算法的对偶形式

基本推导：对偶形式的基本想法是，将w和b表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得w和b

原始形式中，更新通过$w\leftarrow w+\eta y_ix_i$和$b\leftarrow b+\eta y_i$进行，修改n次后得到
$$
w=\sum_{i=1}^{N}n_i\eta y_ix_i\qquad \qquad b=\sum_{i=1}^{N}n_i\eta y_i
$$
$n_i$表示样本点$(x_i,y_i)$在更新过程中被使用了$n_i$次，$n_i$值越大说明该点被更新次数越多，说明该点离平面越近越不好分。将上式代入到原始模型可得
$$
f(x)=sign(w\cdot x+b)=sign(\sum_{j=1}^{N}n_j\eta y_jx_j\cdot x+\sum_{j=1}^{N}n_j\eta y_j)
$$
此时学习目标不再是$w,b$，而变成$n_i$，令$a_i=n_i\eta $，则感知机模型变成
$$
f(x)=sign(\sum_{j=1}^{N}a_jy_jx_j\cdot x+b)
$$


输入：线性可分训练数据集$T＝{(x_1 ，y_1 ),(x_2 ,y_2 ),…,(x_N ,y_N )}$，其中$y_i\in\{-1,+1\}$，i＝1,2,…,N；学习率 $0<\eta\leqq 1$

输出：$a，b$，其中$a=(a_1,a_2,……,a_n)^T$

感知机模型：$f(x)＝sign(\sum_{j=1}^{n}a_jy_jx_j\cdot x+b)$

损失函数（策略）：$\min_{w,b} L(w,b)=-\sum_{x_i\in M} y_i (\sum_{j=1}^{N}a_jy_jx_j\cdot x_i+\sum_{j=1}^{N}n_j\eta y_j)$

步骤：

（1）选取初值$a=0,b=0$
（2）在训练集中选取数据$(x_i ，y_i )$（注意$x_i$表数据，$y_i$为标签）
（3）如果$y_i (\sum_{j=1}^{N}a_jy_jx_j\cdot x_i+b)≤0$，即该点为误分类点
$$
a_i\leftarrow a_i+\eta(相当于n_i=n_i+1)     \qquad b\leftarrow b+\eta y_i(y_i为该数据的标签值1或-1)
$$
（4）转至（2），直至训练集中没有误分类点

对偶形式中训练实例仅以内积的形式出现。为了方便，可以预先将训练集中实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的Gram矩阵
$$
G=[x_i^T\cdot x_j]_{N\times N}
$$
例：有$x_1=(3,3)^T,x_2=(4,3)^T,x_3=(1,1)^T$，$T=(x_1,1),(x_2,1),(x_3,-1)$，$\eta=1$
$$
G = \left[
\matrix{
  x_1^T\cdot x_1 & x_1^T\cdot x_2 & x_1^T\cdot x_3\\
  x_2^T\cdot x_1 & x_2^T\cdot x_2 & x_2^T\cdot x_3\\
  x_3^T\cdot x_1 & x_3^T\cdot x_2 & x_3^T\cdot x_3 
}
\right]
$$

<img src="images/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%20%E6%84%9F%E7%9F%A5%E6%9C%BA/image-20211102173605200.png" alt="image-20211102173605200" style="zoom:67%;" />

<img src="images/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%20%E6%84%9F%E7%9F%A5%E6%9C%BA/image-20211102173638321.png" alt="image-20211102173638321" style="zoom:67%;" />


## 课后习题

2.1Minsky与Papert指出：感知机因为是线性模型，所以不能表示复杂函数，如异或（XOR）。验证感知机为什么不能表示异或

例如X异或Y，在二维平面上，无法找到一条直线将四个样例区分开

2.2实现一个感知机





## 参考文献

1. [统计学习方法习题解答](https://datawhalechina.github.io/statistical-learning-method-solutions-manual/#/chapter01/chapter01?id=%e4%b9%a0%e9%a2%9812)
2. [lihang-code](https://github.com/fengdu78/lihang-code/blob/master/%E7%AC%AC01%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA/1.Introduction_to_statistical_learning_methods.ipynb)
3. [感知机对偶形式推导过程](https://www.cnblogs.com/qiu-hua/p/12755378.html)

