# 第七章 预备知识拉格朗日对偶性

在约束最优化问题中，常常利用拉格朗日对偶性将原始问题转化为对偶问题，通过求解对偶问题获得原始问题的解。该方法应用在许多统计学方法中，如最大熵模型、支持向量机。

## 1.原始问题

假设$f(x),c_i(x),h_j(x)$是定义在Rn上的连续可微函数。考虑如下约束最优化问题

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185116078.png" alt="image-20211114185116078" style="zoom:67%;" />

称此约束最优化问题为原始最优化问题或原始问题。引进广义拉格朗日函数：
$$
L(x,α,β)=f(x)+\sum_{i=1}^{k}α_ic_i(x)+\sum_{j=1}^{l}β_jh_j(x)
$$
这里, αi，βj是拉格朗日乘子，αi≥0，x为输入向量，考虑x的函数
$$
θ_P(x)=\max_{α,β;α_i≥0}L(x,α,β)
$$
容易得到，在满足C.2,C.3的约束条件时，$θ_P(x)=f(x)$，所以如果考虑极小化问腿
$$
\min_xθ_P(x)=\min_x\max_{α,β;α_i≥0}L(x,α,β)
$$
它是与原始最优化问题（C.1)~C.3）等价的，这样⼀来，就把原始最优化问题表示为广义拉格朗日函数的极小
极大问题。为了方便，定义原始问题的最优值$P^*=\min_xθ_p(x)$

## 2.对偶问题

定义：$θ_D(α,β)=\min_{x}L(x,α,β)$，再考虑极大化$θ_D(α,β)$，即
$$
\max_{α,β;α_i≥0}θ_D(α,β)=\max_{α,β;α_i≥0}\min_{x}L(x,α,β)
$$
上式右边问题的求解被称为广义拉格朗日函数的极大极小问题，可以将广义拉格朗日函数的极大极小问题表示为约束最优化问题：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185126469.png" alt="image-20211114185126469" style="zoom:67%;" />

称为原始问题的对偶问题。定义对偶问题的最优值$d^*=\max_{α,β;α_i≥0}θ_D(α,β)$，称为对偶问题的值

## 3.原始问题对偶问题关系

**定理一：**若原始问题和对偶问题都有最优值，则
$$
d^*=\max_{α,β;α_i≥0}\min_{x}L(x,α,β)≤\min_x\max_{α,β;α_i≥0}L(x,α,β)=p^*
$$

**定理二：**假设函数f(x)和ci(x)是凸函数，hj(x)是仿射函数，并且不等式约束ci(x)是严格可行的, 则x∗和α∗,分别是原始问题和对偶问题的解的充分必要条件是x∗,α∗,β∗满足KKT条件：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185135100.png" alt="image-20211114185135100" style="zoom:67%;" />

# 第七章 支持向量机（SVM）

## 本章概要

1. 支持向量机是⼀种二类分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机
2. 支持向量机学习方法包含构建由简至繁的模型：线性可分支持向量机、线性支持向量机及非线性支持向量机

## 线性可分支持向量机与硬间隔最大化

当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个。线性可分支持向量机利用间隔最大化求最优分离超平面，这时，解是唯⼀的。

### 线性可分支持向量机

给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为$w^*\cdot x+b^*=0$，相应的分类决策函数$f(x)=sign(w^*\cdot x+b^*)$，称为线性可分支持向量机

### 函数间隔和几何间隔

**函数间隔**（可以表示分类预测的正确性及确信度）：对于给定的训练数据集T和超平⾯(w,b)，定义超平面(w,b)关于样本点$(x_i ，y_i )$的函数间隔为$γ_i=y_i(w\cdot x+b)$（γhat），定义超平面(w,b)关于训练数据集T的函数间隔为超平面(w,b)关于T中所有样本点$(x_i ，y_i )$的函数间隔之最小值

**几何间隔**：因为只要成比例地改变w和b，例如将它们改为2w和2b，超平面并没有改变，但函数间隔却成为原来的2倍。要求||w||＝ 1，使得间隔是确定的。这时函数间隔成为几何间隔，一般当样本点被正确分类时，点$x_i$与平面的距离为$γ_i=y_i(\frac{w}{||w||}\cdot x+\frac{b}{||w||})$，||w||是w的L2范数

### 间隔最大化（硬间隔）

对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。这样的超平面应该对未知的新实例有很好
的分类预测能力

具体地，求几何间隔最大可以表示为下面约束最优化问题：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185145111.png" alt="image-20211114185145111" style="zoom:67%;" />

考虑几何间隔和函数间隔的关系，可改写为：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185155901.png" alt="image-20211114185155901" style="zoom:75%;" />

函数间隔γhat的取值并不影响最优化问题的解，可以取值为1，最大化$\frac{1}{||w||}$和最小化$\frac{1}{2}||w||^2$等价，所以可以转化为：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185208912.png" alt="image-20211114185208912" style="zoom:70%;" />

综上所述，就有下面的线性可分支持向量机的学习算法——**最大间隔法**

**输入：**线性可分数据集$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\};y_i∈\{-1,+1\}$

**输出：**最大间隔分离超平面$w^*\cdot x+b^*=0$和分类决策函数$f(x)=sign(w^*\cdot x+b^{*})$

**支持向量**：在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例，它使等号成立

例题：

​      图片

### 学习的对偶算法

应用拉格朗日对偶性，**通过求解对偶问题得到原始问题的最优解**，这就是线性可分支持向量机的对偶算法。这样做的优点是对偶问题往往更容易求解其次是自然引入核函数，进而推广到非线性分类问题

**步骤：**

1. 首先构建拉格朗日函数，对每⼀个不等式约束引进拉格朗日乘子$a_i≥0，i＝1,2,…,N$，定义拉格朗日函数，a＝(a1 ,a2 ,…,aN ) T 为拉格朗日向量，$a_i≥0$
   $$
   L(w,b,a)=\frac{1}{2}||w||^2-\sum_{i=1}^{N}a_iy_i(w\cdot x_i+b)+\sum_{i=1}^{N}a_i
   $$

2. 根据拉格朗日对偶性，**原始问题的对偶问题是极大极小问题**：所以，为了得到对偶问题的解，需要先求L(w,b,a)对w,b的极小，再求对a的极大

3. 求$\min_{w,b}L(w,b,a)$，分别对w，b求偏导并令之等于0

   <img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185222048.png" alt="image-20211114185222048" style="zoom:67%;" />

   将上式代入拉格朗日函数，得到：
   $$
   L(w,b,a)=\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}a_ia_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^{N}a_iy_i((\sum_{j=1}^{N}a_jy_jx_j)\cdot x_i+b)+\sum_{i=1}^{N}a_i
   $$
   即：
   $$
                           \min_{w,b} L(w,b,a)=-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}a_ia_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^{N}a_i
   $$

4. 求$\min_{w,b}L(w,b,a)$对a的极大，就是对偶问题，负号去掉转化成求极小

   

5. 对线性可分训练数据集，假设**对偶最优化问题**对a的解为$a^* ＝(a^*_1,a^*_2 ,…, a^*_n)^T$，则存在下标j，使得$a^*_j>0$，并可按下式求得**原始最优化问题**的解w * ,b * :

   <img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185232111.png" alt="image-20211114185232111" style="zoom:60%;" />

   **KKT条件**成立即得：
   $$
   \nabla_wL(w^*,b^*,a^*)=w^*-\sum_{i=1}^{N}a^*_iy_ix_i=0
   \\\nabla_bL(w^*,b^*,a^*)=-\sum_{i=1}^{N}a^*_iy_i=0
   \\a^*_i(y_i(w^*\cdot x_i+b^*)-1)=0,i=1,2,\cdots,N
   \\y_i(w^*\cdot x_i+b^*)-1≥0,i=1,2,\cdots,N
   \\a^*_i≥0,i=1,2,\cdots,N
   $$
   由此得到：$w^*=\sum_ia^*_iy_ix_i$，因为$y^2_i=1$，所以对上式第四行变换可得到：$b^*=y_j-\sum_{i=1}^{N}a^*_iy_i(x_ix_j)$，由此可知，分离超平面和分离决策函数可以写成:
   $$
   \sum_{i=1}^{N}a^*_iy_i(x\cdot x_i)+b^*=0
   \\f(x)=sign(\sum_{i=1}^{N}a^*_iy_i(x\cdot x_i)+b^*)
   $$
   这就是说，分类决策函数只依赖于输入x和训练样本输入的内积。上式称为线性可分支持向量机的对偶形式

例7.2



对于线性可分问题上述算法完美，但是一般训练数据集往往是线性不可分的，即在样本中出现噪声或特异点，此时有更一般的算法

## 线性支持向量机与软间隔最大化

怎么才能将线性可分的学习方法扩展到线性不可分问题呢？这就需要修改硬间隔最大化，使其成为软间隔最大化。

线性不可分意味着某些样本点$(x_i ，y_i )$不能满足函数间隔大于等于1的约束条件。为了解决这个问题可以对每个样本点$(x_i ，y_i )$引进⼀个松弛变量$ ξ_i≥0$，使函数间隔加上松弛变量⼤于等于1。这样，约束条件变为：
$$
y_i(w\cdot x_i+b)≥1-ξ_i
$$
同时目标函数由原来的$\frac{1}{2}||w||^{2}$变成$\frac{1}{2}||w||^{2}+C\sum_{i=1}^{N}ξ_i$，C>0，称为惩罚参数，C值大时对误分类的惩罚增大，C值小时对误分类的惩罚减小

线性不可分的线性支持向量机的学习问题的原始问题即为下式，可以证明w解唯一，b的解存在于一个区间

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171405897.png" alt="image-20211116171405897" style="zoom:80%;" />

### 线性支持向量机

对于给定的**线性不可分**的训练数据集，通过求解凸⼆次规划问题，即软间隔最大化问题7.32-7.34，得到的分离超平面为$w^*\cdot x+b^*=0$，相应的分类决策函数为$f(x)=sign(w^*\cdot x+b^{*})$

### 学习的对偶算法分析

原始问题7.32-7.34的对偶问题即为

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171445812.png" alt="image-20211116171445812" style="zoom:80%;" />

原始最优化问题7.32〜7.34的拉格朗日函数是下式7.40，其中$α_i≥0，μ_i≥0$
$$
L(w,b,ξ,α,μ)=\frac{1}{2}||w||^{2}+C\sum_{i=1}^{N}ξ_i-\sum_{i=1}^{N}α_i(y_i(w\cdot x+b)-1+ξ_i)-\sum_{μ=1}^{N}μ_iξ_i
$$
对偶问题是拉格朗日函数的极大极小问题。首先求L(w，b, ξ，α，μ )对w,b,ξ 的极小，由

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171513210.png" alt="image-20211116171513210" style="zoom:70%;" />

得到：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171552731.png" alt="image-20211116171552731" style="zoom:70%;" />

将7.41-7.43带回到7.40，得到：
$$
\min_{w,b,ξ}L(w,b,ξ,α,μ)=-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}α_iα_jy_iy_j(x_I\cdot x_j)+\sum_{i=1}^{N}α_i
$$
再对$\min_{w,b,ξ}L(w,b,ξ,α,μ)$求对α的极大，即得到对偶问题：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171623204.png" alt="image-20211116171623204" style="zoom:80%;" />

利用等式7.46消去$μ_i$，并将约束7.46-7.48写成$0≤α_i≤C$，再将对目标函数求极大转换为求极小，就得到对偶问题7.37-7.39

**定理7.3**  设$a^* ＝(a_1^* ,a_2^* ,…,a_N^* )^T $是对偶问题7.37〜7.39的⼀个解，若存在$a^*$ 的⼀个分量$a_j^*$ ，0<$a_j^*$ <C，则原始问题7.32
〜7.34的解w * ,b * 可按下式求得：
$$
w^*=\sum_{i=1}^{N}α_i^*y_ix_i
\\b^*=y_j-\sum_{i=1}^{N}y_iα_i^*(x_i\cdot x_j)
$$
分离超平面和分类决策函数即为：
$$
\sum_{i=1}^{N}a^*_iy_i(x\cdot x_i)+b^*=0
\\f(x)=sign(\sum_{i=1}^{N}a^*_iy_i(x\cdot x_i)+b^*)
$$

### 线性支持向量机学习算法

输⼊：训练数据集$T＝\{(x 1 ，y 1 ),(x 2 ，y 2 ),…,(x N ,y N )\}，$其中，$x i ∊x＝R^n ，y i ∊ ＝{-1,+1}，i＝1,2,…,N；$
输出：分离超平面和分类决策函数

1. 选择惩罚参数C>0，构造并求解凸⼆次规划问题，求解最优解$α^*=(α^*_1,α^*_2,……，α^*_N)$

   <img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116150010003.png" alt="image-20211116150010003" style="zoom:80%;" />

2. 计算$w^*=\sum_{i=1}^{N}α_i^*y_ix_i$，选择a* 的⼀个分量$α_j^*$适合条件0< $α_j^*$<C，计算$b^*=y_j-\sum_{i=1}^{N}y_iα_i^*(x_i\cdot x_j)$

3. 求得分离超平面和分类决策函数
   $$
   w^*\cdot x+b^*=0
   \\f(x)=sign(w^*\cdot x+b^*)
   $$
   

### 支持向量

在线性不可分的情况下，将对偶问题（7.37）〜（7.39）的解$a^* ＝(a_1^* ,a_2^* ,…,a_N^* )^T $ 中对应于 $α_i^*$>0的样本点$(x_i ，y_i )$的实例xi称为软间隔的支持向量

### 合页损失函数

线性支持向量机学习还有另外⼀种解释，就是最小化以下目标函数，

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171743986.png" alt="image-20211116171743986" style="zoom:70%;" />

函数$L(y(w\cdot x+b))=[1-y(w\cdot x+b)]_+$，称为合页损失函数，下标+表示取正值函数

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171759707.png" alt="image-20211116171759707" style="zoom:67%;" />

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171822198.png" alt="image-20211116171822198" style="zoom:67%;" />

等价于最优化问题$\min_{w,b}\sum_{i=1}^{N}[1-y(w\cdot x+b)]_++λ||w||^{2}$

## 非线性支持向量机与核函数

### 核技巧

非线性分类问题是指通过利用非线性模型才能很好地进行分类的问题，如下图（其实就是用超曲面划分）：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171850619.png" alt="image-20211116171850619" style="zoom:67%;" />

所采取的方法是进行⼀个非线性变换，将非线性问题变换为线性问题，步骤如下，**核技巧便是这样的方法**：

1. 使用一个变换将原空间的数据映射到新空间
2. 然后在新空间里线性分类学习方法从训练数据中学习分类模型

**核函数**：设x是输入空间（欧式空间），又设为特征空间（希尔伯特空间），如果存在⼀个从x到H的映射

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171907067.png" alt="image-20211116171907067" style="zoom:67%;" />

使得对所有x,z∊x，函数K(x,z)满足条件

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171917907.png" alt="image-20211116171917907" style="zoom:67%;" />

则称K(x,z)为核函数，Ø(x)为映射函数

### 核技巧在支持向量机中的应用

在对偶问题的目标函数（7.37）中的内积xi ·xj 可以用核函数K(xi ，xj )＝Ø(xi)·Ø(xj)来代替，此时对偶问题的目标函数变成
$$
W(α)=\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}α_iα_jy_iy_jK(x_i,x_j)-\sum_{j=1}^{N}α_i
$$
同样，分类决策函数中的内积也可以用核函数代替，而分类决策函数式成为
$$
f(x)=sign(\sum_{i=1}^{N}a^*_iy_i(Ø(x_i)\cdot Ø(x)+b^*)=sign(\sum_{i=1}^{N}a^*_iy_iK(x_i,x)+b^*)
$$

### 正定核

本节叙述正定核的充要条件。通常所说的核函数就是正定核函数

K(x,z)为正定核函数的充要条件是对任意$x_i∊X$ ，i＝1,2,…,m，K(x,z)对应的Gram矩阵$K=[K(x_i,x_j)]_{m✖m}$是半正定阵

### 常用核函数

1. 多项式核函数，对应的支持向量机是一个p次多项式分类器
   $$
   K(x,z)=(x\cdot z+1)^p
   \\f(x)=sign(\sum_{i=1}^{N}α_i^*y_i(x_i\cdot x+1)^p+b^*)
   $$
   

2.  高斯核函数
   $$
   K(x,z)=exp(-\frac{||x-z||^2}{2σ^2})
   \\f(x)=sign(\sum_{i=1}^{N}α_i^*y_iexp(-\frac{||x-z||^2}{2σ^2})+b^*)
   $$
   

3. 字符串核函数

   核函数不仅可以定义在欧式空间上，还可以定义在离散数据的集合上。比如，字符串核是定义在字符串集合上的核函数。字符串核函数在文本分类、信息检索、生物信息学等方面都有应用。

### 非线性支持向量机学习算法

输⼊：训练数据集$T＝\{(x 1 ，y 1 ),(x 2 ，y 2 ),…,(x N ,y N )\}$，其中$x_i∊x＝R^n ，y_i ∊ ＝{-1,+1}，i＝1,2,…,N；$
输出：分类决策函数

1. 选取适当的核函数K(x,z)和适当的参数C，构造并求解最优化问题,，求得最优解$a^* ＝(a_1^* ,a_2^* ,…,a_N^* )^T $ 

   <img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116172026895.png" alt="image-20211116172026895" style="zoom:65%;" />

2. 选择a* 的⼀个正分量$0<α_j^*<C$，计算
   $$
   b^*=y_j-\sum_{i=1}^{N}α_i^*y_iK(x_i\cdot x_j)
   $$

3. 构造决策函数
   $$
   f(x)=sign(\sum_{i=1}^{N}α_i^*y_iK(x\cdot x_i)+b^*)
   $$
   当K(x,z)是正定核函数时，问题7.95〜7.97是凸⼆次规划问题，解是存在的

## 序列最小最优化算法（SMO）

本节讲述其中的序列最小最优化（SMO）算法，SMO算法要解如下凸⼆次规划的对偶问题：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116172116537.png" alt="image-20211116172116537" style="zoom:67%;" />

SMO算法是⼀种启发式算法，是将大优化问题分解成多个小优化问题来求解。目标是求出一系列α和b，从而求出权重w

**SMO算法包括两个部分**：求解两个变量⼆次规划的解析方法和选择变量的启发式方法

**SMO的工作原理：**

每次循环中选择两个α进行优化处理，一旦找到一对合适（两个α要符合一定的条件）的α，那么就增大其中一个减小另一个

### 两个变量二次规划的求解方法

假设选择的两个变量是a1 ,a2 ，其他变量ai (i＝3,4,…,N)是固定的。于是SMO的最优化问题7.98〜7.100的子问题可以写成：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116172131744.png" alt="image-20211116172131744" style="zoom:67%;" />

## 简化版SMO实现
