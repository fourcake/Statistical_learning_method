## 第七章 预备知识拉格朗日对偶性

在约束最优化问题中，常常利用拉格朗日对偶性将原始问题转化为对偶问题，通过求解对偶问题获得原始问题的解。该方法应用在许多统计学方法中，如最大熵模型、支持向量机。

### 1.原始问题

假设$f(x),c_i(x),h_j(x)$是定义在Rn上的连续可微函数。考虑如下约束最优化问题

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185116078.png" alt="image-20211114185116078" style="zoom:67%;" />

称此约束最优化问题为原始最优化问题或原始问题。引进广义拉格朗日函数：
$$
L(x,α,β)=f(x)+\sum_{i=1}^{k}α_ic_i(x)+\sum_{j=1}^{l}β_jh_j(x)
$$
这里, αi，βj是拉格朗日乘子，αi≥0，x为输入向量，考虑x的函数
$$
θ_P(x)=\max_{α,β;α_i≥0}L(x,α,β)
$$
容易得到，在满足C.2,C.3的约束条件时，$θ_P(x)=f(x)$，所以如果考虑极小化问腿
$$
\min_xθ_P(x)=\min_x\max_{α,β;α_i≥0}L(x,α,β)
$$
它是与原始最优化问题（C.1)~C.3）等价的，这样⼀来，就把原始最优化问题表示为广义拉格朗日函数的极小
极大问题。为了方便，定义原始问题的最优值$P^*=\min_xθ_p(x)$

### 2.对偶问题

定义：$θ_D(α,β)=\min_{x}L(x,α,β)$，再考虑极大化$θ_D(α,β)$，即
$$
\max_{α,β;α_i≥0}θ_D(α,β)=\max_{α,β;α_i≥0}\min_{x}L(x,α,β)
$$
上式右边问题的求解被称为广义拉格朗日函数的极大极小问题，可以将广义拉格朗日函数的极大极小问题表示为约束最优化问题：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185126469.png" alt="image-20211114185126469" style="zoom:67%;" />

称为原始问题的对偶问题。定义对偶问题的最优值$d^*=\max_{α,β;α_i≥0}θ_D(α,β)$，称为对偶问题的值

### 3.原始问题对偶问题关系

**定理一：**若原始问题和对偶问题都有最优值，则
$$
d^*=\max_{α,β;α_i≥0}\min_{x}L(x,α,β)≤\min_x\max_{α,β;α_i≥0}L(x,α,β)=p^*
$$

**定理二：**假设函数f(x)和ci(x)是凸函数，hj(x)是仿射函数，并且不等式约束ci(x)是严格可行的, 则x∗和α∗,分别是原始问题和对偶问题的解的充分必要条件是x∗,α∗,β∗满足KKT条件：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185135100.png" alt="image-20211114185135100" style="zoom:67%;" />

## 第七章 支持向量机（SVM）

### 本章概要

1. 支持向量机是⼀种二类分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机
2. 支持向量机学习方法包含构建由简至繁的模型：线性可分支持向量机、线性支持向量机及非线性支持向量机

### 线性可分支持向量机与硬间隔最大化

当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个。线性可分支持向量机利用间隔最大化求最优分离超平面，这时，解是唯⼀的。

#### 线性可分支持向量机

给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为$w^*\cdot x+b^*=0$，相应的分类决策函数$f(x)=sign(w^*\cdot x+b^*)$，称为线性可分支持向量机

#### 函数间隔和几何间隔

**函数间隔**（可以表示分类预测的正确性及确信度）：对于给定的训练数据集T和超平⾯(w,b)，定义超平面(w,b)关于样本点$(x_i ，y_i )$的函数间隔为$γ_i=y_i(w\cdot x+b)$（γhat），定义超平面(w,b)关于训练数据集T的函数间隔为超平面(w,b)关于T中所有样本点$(x_i ，y_i )$的函数间隔之最小值

**几何间隔**：因为只要成比例地改变w和b，例如将它们改为2w和2b，超平面并没有改变，但函数间隔却成为原来的2倍。要求||w||＝ 1，使得间隔是确定的。这时函数间隔成为几何间隔，一般当样本点被正确分类时，点$x_i$与平面的距离为$γ_i=y_i(\frac{w}{||w||}\cdot x+\frac{b}{||w||})$，||w||是w的L2范数

#### 间隔最大化（硬间隔）

对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。这样的超平面应该对未知的新实例有很好
的分类预测能力

具体地，求几何间隔最大可以表示为下面约束最优化问题：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185145111.png" alt="image-20211114185145111" style="zoom:67%;" />

考虑几何间隔和函数间隔的关系，可改写为：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185155901.png" alt="image-20211114185155901" style="zoom:75%;" />

函数间隔γhat的取值并不影响最优化问题的解，可以取值为1，最大化$\frac{1}{||w||}$和最小化$\frac{1}{2}||w||^2$等价，所以可以转化为：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185208912.png" alt="image-20211114185208912" style="zoom:70%;" />

综上所述，就有下面的线性可分支持向量机的学习算法——**最大间隔法**

**输入：**线性可分数据集$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\};y_i∈\{-1,+1\}$

**输出：**最大间隔分离超平面$w^*\cdot x+b^*=0$和分类决策函数$f(x)=sign(w^*\cdot x+b^{*})$

**支持向量**：在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例，它使等号成立

例题：

​      图片

#### 学习的对偶算法

应用拉格朗日对偶性，**通过求解对偶问题得到原始问题的最优解**，这就是线性可分支持向量机的对偶算法。这样做的优点是对偶问题往往更容易求解其次是自然引入核函数，进而推广到非线性分类问题

**步骤：**

1. 首先构建拉格朗日函数，对每⼀个不等式约束引进拉格朗日乘子$a_i≥0，i＝1,2,…,N$，定义拉格朗日函数，a＝(a1 ,a2 ,…,aN ) T 为拉格朗日向量，$a_i≥0$
   $$
   L(w,b,a)=\frac{1}{2}||w||^2-\sum_{i=1}^{N}a_iy_i(w\cdot x_i+b)+\sum_{i=1}^{N}a_i
   $$

2. 根据拉格朗日对偶性，**原始问题的对偶问题是极大极小问题**：所以，为了得到对偶问题的解，需要先求L(w,b,a)对w,b的极小，再求对a的极大

3. 求$\min_{w,b}L(w,b,a)$，分别对w，b求偏导并令之等于0

   <img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185222048.png" alt="image-20211114185222048" style="zoom:67%;" />

   将上式代入拉格朗日函数，得到：
   $$
   L(w,b,a)=\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}a_ia_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^{N}a_iy_i((\sum_{j=1}^{N}a_jy_jx_j)\cdot x_i+b)+\sum_{i=1}^{N}a_i
   $$
   即：
   $$
                           \min_{w,b} L(w,b,a)=-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}a_ia_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^{N}a_i
   $$

4. 求$\min_{w,b}L(w,b,a)$对a的极大，就是对偶问题，负号去掉转化成求极小

   

5. 对线性可分训练数据集，假设**对偶最优化问题**对a的解为$a^* ＝(a^*_1,a^*_2 ,…, a^*_n)^T$，则存在下标j，使得$a^*_j>0$，并可按下式求得**原始最优化问题**的解w * ,b * :

   <img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185232111.png" alt="image-20211114185232111" style="zoom:60%;" />

   **KKT条件**成立即得：
   $$
   \nabla_wL(w^*,b^*,a^*)=w^*-\sum_{i=1}^{N}a^*_iy_ix_i=0
   \\\nabla_bL(w^*,b^*,a^*)=-\sum_{i=1}^{N}a^*_iy_i=0
   \\a^*_i(y_i(w^*\cdot x_i+b^*)-1)=0,i=1,2,\cdots,N
   \\y_i(w^*\cdot x_i+b^*)-1≥0,i=1,2,\cdots,N
   \\a^*_i≥0,i=1,2,\cdots,N
   $$
   由此得到：$w^*=\sum_ia^*_iy_ix_i$，因为$y^2_i=1$，所以对上式第四行变换可得到：$b^*=y_j-\sum_{i=1}^{N}a^*_iy_i(x_ix_j)$，由此可知，分离超平面和分离决策函数可以写成:
   $$
   \sum_{i=1}^{N}a^*_iy_i(x\cdot x_i)+b^*=0
   \\f(x)=sign(\sum_{i=1}^{N}a^*_iy_i(x\cdot x_i)+b^*)
   $$
   这就是说，分类决策函数只依赖于输入x和训练样本输入的内积。上式称为线性可分支持向量机的对偶形式

例7.2

