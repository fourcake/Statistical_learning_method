# 第七章 预备知识拉格朗日对偶性

在约束最优化问题中，常常利用拉格朗日对偶性将原始问题转化为对偶问题，通过求解对偶问题获得原始问题的解。该方法应用在许多统计学方法中，如最大熵模型、支持向量机。

## 1.原始问题

假设$f(x),c_i(x),h_j(x)$是定义在Rn上的连续可微函数。考虑如下约束最优化问题

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185116078.png" alt="image-20211114185116078" style="zoom:67%;" />

称此约束最优化问题为原始最优化问题或原始问题。引进广义拉格朗日函数：
$$
L(x,α,β)=f(x)+\sum_{i=1}^{k}α_ic_i(x)+\sum_{j=1}^{l}β_jh_j(x)
$$
这里, αi，βj是拉格朗日乘子，αi≥0，x为输入向量，考虑x的函数
$$
θ_P(x)=\max_{α,β;α_i≥0}L(x,α,β)
$$
容易得到，在满足C.2,C.3的约束条件时，$θ_P(x)=f(x)$，所以如果考虑极小化问腿
$$
\min_xθ_P(x)=\min_x\max_{α,β;α_i≥0}L(x,α,β)
$$
它是与原始最优化问题（C.1)~C.3）等价的，这样⼀来，就把原始最优化问题表示为广义拉格朗日函数的极小
极大问题。为了方便，定义原始问题的最优值$P^*=\min_xθ_p(x)$

## 2.对偶问题

定义：$θ_D(α,β)=\min_{x}L(x,α,β)$，再考虑极大化$θ_D(α,β)$，即
$$
\max_{α,β;α_i≥0}θ_D(α,β)=\max_{α,β;α_i≥0}\min_{x}L(x,α,β)
$$
上式右边问题的求解被称为广义拉格朗日函数的极大极小问题，可以将广义拉格朗日函数的极大极小问题表示为约束最优化问题：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185126469.png" alt="image-20211114185126469" style="zoom:67%;" />

称为原始问题的对偶问题。定义对偶问题的最优值$d^*=\max_{α,β;α_i≥0}θ_D(α,β)$，称为对偶问题的值

## 3.原始问题对偶问题关系

**定理一：**若原始问题和对偶问题都有最优值，则$d^*=\max_{α,β;α_i≥0}\min_{x}L(x,α,β)≤\min_x\max_{α,β;α_i≥0}L(x,α,β)=p^*$

**定理二：**假设函数f(x)和ci(x)是凸函数，hj(x)是仿射函数，并且不等式约束ci(x)是严格可行的, 则x∗和α∗,分别是原始问题和对偶问题的解的充分必要条件是x∗,α∗,β∗满足KKT条件：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185135100.png" alt="image-20211114185135100" style="zoom:67%;" />

# 第七章 支持向量机（SVM）

## 本章概要

1. 支持向量机是⼀种二类分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机
2. 支持向量机学习方法包含构建由简至繁的模型：线性可分支持向量机、线性支持向量机及非线性支持向量机

## 线性可分支持向量机与硬间隔最大化

当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个。线性可分支持向量机利用间隔最大化求最优分离超平面，这时，解是唯⼀的。

### 线性可分支持向量机

给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为$w^*\cdot x+b^*=0$

相应的分类决策函数$f(x)=sign(w^*\cdot x+b^*)$，称为线性可分支持向量机

### 函数间隔和几何间隔

**函数间隔**（可以表示分类预测的正确性及确信度）：对于给定的训练数据集T和超平⾯(w,b)，定义超平面(w,b)关于样本点$(x_i ，y_i )$的函数间隔为$γ_i=y_i(w\cdot x+b)$（γhat），定义超平面(w,b)关于训练数据集T的函数间隔为超平面(w,b)关于T中所有样本点$(x_i ，y_i )$的函数间隔之最小值

**几何间隔**：因为只要成比例地改变w和b，例如将它们改为2w和2b，超平面并没有改变，但函数间隔却成为原来的2倍。要求||w||＝ 1，使得间隔是确定的。这时函数间隔成为几何间隔，一般当样本点被正确分类时，点$x_i$与平面的距离为$γ_i=y_i(\frac{w}{||w||}\cdot x+\frac{b}{||w||})$，||w||是w的L2范数

### 间隔最大化（硬间隔）

对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。这样的超平面应该对未知的新实例有很好
的分类预测能力

具体地，求几何间隔最大可以表示为下面约束最优化问题：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185145111.png" alt="image-20211114185145111" style="zoom:67%;" />

考虑几何间隔和函数间隔的关系，可改写为：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185155901.png" alt="image-20211114185155901" style="zoom:75%;" />

函数间隔γhat的取值并不影响最优化问题的解，可以取值为1，最大化$\frac{1}{||w||}$和最小化$\frac{1}{2}||w||^2$等价，所以可以转化为：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185208912.png" alt="image-20211114185208912" style="zoom:70%;" />

综上所述，就有下面的线性可分支持向量机的学习算法——**最大间隔法**

**输入：**线性可分数据集$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\};y_i∈\{-1,+1\}$

**输出：**最大间隔分离超平面$w^*\cdot x+b^*=0$和分类决策函数$f(x)=sign(w^*\cdot x+b^{*})$

**支持向量**：在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例，它使等号成立

例题：

​      图片

### 学习的对偶算法

应用拉格朗日对偶性，**通过求解对偶问题得到原始问题的最优解**，这就是线性可分支持向量机的对偶算法。这样做的优点是对偶问题往往更容易求解其次是自然引入核函数，进而推广到非线性分类问题

**步骤：**

1. 首先构建拉格朗日函数，对每⼀个不等式约束引进拉格朗日乘子$a_i≥0，i＝1,2,…,N$，定义拉格朗日函数，a＝(a1 ,a2 ,…,aN ) T 为拉格朗日向量，$a_i≥0$
   $$
   L(w,b,a)=\frac{1}{2}||w||^2-\sum_{i=1}^{N}a_iy_i(w\cdot x_i+b)+\sum_{i=1}^{N}a_i
   $$

2. 根据拉格朗日对偶性，**原始问题的对偶问题是极大极小问题**：所以，为了得到对偶问题的解，需要先求L(w,b,a)对w,b的极小，再求对a的极大

3. 求$\min_{w,b}L(w,b,a)$，分别对w，b求偏导并令之等于0

   <img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185222048.png" alt="image-20211114185222048" style="zoom:67%;" />

   将上式代入拉格朗日函数，得到：
   $$
   L(w,b,a)=\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}a_ia_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^{N}a_iy_i((\sum_{j=1}^{N}a_jy_jx_j)\cdot x_i+b)+\sum_{i=1}^{N}a_i
   $$
   即：
   $$
                           \min_{w,b} L(w,b,a)=-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}a_ia_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^{N}a_i
   $$

4. 求$\min_{w,b}L(w,b,a)$对a的极大，就是对偶问题，负号去掉转化成求极小

   

5. 对线性可分训练数据集，假设**对偶最优化问题**对a的解为$a^* ＝(a^*_1,a^*_2 ,…, a^*_n)^T$，则存在下标j，使得$a^*_j>0$，并可按下式求得**原始最优化问题**的解w * ,b * :

   <img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211114185232111.png" alt="image-20211114185232111" style="zoom:60%;" />

   **KKT条件**成立即得：
   
   <img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116172757759.png" alt="image-20211116172757759" style="zoom:80%;" />
   
   由此得到：$w^*=\sum_i a^*_i y_i x_i$，因为$y^2_i=1$，所以对上式第四行变换可得到：$b^*=y_j-\sum_{i=1}^{N}a^*_i y_i(x_ix_j)$，由此可知，分离超平面和分离决策函数可以写成:
   
   <img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116172842829.png" alt="image-20211116172842829" style="zoom:75%;" />
   
   这就是说，分类决策函数只依赖于输入x和训练样本输入的内积。上式称为线性可分支持向量机的对偶形式

例7.2



对于线性可分问题上述算法完美，但是一般训练数据集往往是线性不可分的，即在样本中出现噪声或特异点，此时有更一般的算法

## 线性支持向量机与软间隔最大化

怎么才能将线性可分的学习方法扩展到线性不可分问题呢？这就需要修改硬间隔最大化，使其成为软间隔最大化。

线性不可分意味着某些样本点$(x_i ，y_i )$不能满足函数间隔大于等于1的约束条件。为了解决这个问题可以对每个样本点$(x_i ，y_i )$引进⼀个松弛变量$ ξ_i≥0$，使函数间隔加上松弛变量⼤于等于1。这样，约束条件变为：
$$
y_i(w\cdot x_i+b)≥1-ξ_i
$$
同时目标函数由原来的$\frac{1}{2}||w||^{2}$变成$\frac{1}{2}||w||^{2}+C\sum_{i=1}^{N}ξ_i$，C>0，称为惩罚参数，C值大时对误分类的惩罚增大，C值小时对误分类的惩罚减小

线性不可分的线性支持向量机的学习问题的原始问题即为下式，可以证明w解唯一，b的解存在于一个区间

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171405897.png" alt="image-20211116171405897" style="zoom:80%;" />

### 线性支持向量机

对于给定的**线性不可分**的训练数据集，通过求解凸⼆次规划问题，即软间隔最大化问题7.32-7.34，得到的分离超平面为$w^*x+b^*=0$

相应的分类决策函数为$f(x)=sign(w^* x+b^{*})$

### 学习的对偶算法分析

原始问题7.32-7.34的对偶问题即为

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171445812.png" alt="image-20211116171445812" style="zoom:80%;" />

原始最优化问题7.32〜7.34的拉格朗日函数是下式7.40，其中$α_i≥0，μ_i≥0$
$$
L(w,b,ξ,α,μ)=\frac{1}{2}||w||^{2}+C\sum_{i=1}^{N}ξ_i-\sum_{i=1}^{N}α_i(y_i(w\cdot x+b)-1+ξ_i)-\sum_{μ=1}^{N}μ_iξ_i
$$
对偶问题是拉格朗日函数的极大极小问题。首先求L(w，b, ξ，α，μ )对w,b,ξ 的极小，由

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171513210.png" alt="image-20211116171513210" style="zoom:70%;" />

得到：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171552731.png" alt="image-20211116171552731" style="zoom:70%;" />

将7.41-7.43带回到7.40，得到：
$$
\min_{w,b,ξ}L(w,b,ξ,α,μ)=-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}α_iα_jy_iy_j(x_I\cdot x_j)+\sum_{i=1}^{N}α_i
$$
再对$\min_{w,b,ξ}L(w,b,ξ,α,μ)$求对α的极大，即得到对偶问题：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171623204.png" alt="image-20211116171623204" style="zoom:80%;" />

利用等式7.46消去$μ_i$，并将约束7.46-7.48写成$0≤α_i≤C$，再将对目标函数求极大转换为求极小，就得到对偶问题7.37-7.39

**定理7.3**  设$a^* ＝(a_1^* ,a_2^* ,…,a_N^* )^T $是对偶问题7.37〜7.39的⼀个解，若存在$a^*$ 的⼀个分量$a_j^*$ ，0<$a_j^*$ <C，则原始问题7.32
〜7.34的解w * ,b * 可按下式求得：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116172944863.png" alt="image-20211116172944863" style="zoom:75%;" />

分离超平面和分类决策函数即为：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116172958659.png" alt="image-20211116172958659" style="zoom:70%;" />

### 线性支持向量机学习算法

![image-20211116174132060](images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116174132060.png)



### 支持向量

在线性不可分的情况下，将对偶问题（7.37）〜（7.39）的解$a^* ＝(a_1^* ,a_2^* ,…,a_N^* )^T $ 中对应于 $α_i^*$>0的样本点$(x_i ，y_i )$的实例xi称为软间隔的支持向量

### 合页损失函数

线性支持向量机学习还有另外⼀种解释，就是最小化以下目标函数，

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171743986.png" alt="image-20211116171743986" style="zoom:70%;" />

函数$L(y(w\cdot x+b))=[1-y(w\cdot x+b)]_+$，称为合页损失函数，下标+表示取正值函数

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171759707.png" alt="image-20211116171759707" style="zoom:67%;" />

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171822198.png" alt="image-20211116171822198" style="zoom:67%;" />

等价于最优化问题$\min_{w,b}\sum_{i=1}^{N}[1-y(w\cdot x+b)]_++λ||w||^{2}$

## 非线性支持向量机与核函数

### 核技巧

非线性分类问题是指通过利用非线性模型才能很好地进行分类的问题，如下图（其实就是用超曲面划分）：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171850619.png" alt="image-20211116171850619" style="zoom:67%;" />

所采取的方法是进行⼀个非线性变换，将非线性问题变换为线性问题，步骤如下，**核技巧便是这样的方法**：

1. 使用一个变换将原空间的数据映射到新空间
2. 然后在新空间里线性分类学习方法从训练数据中学习分类模型

**核函数**：设x是输入空间（欧式空间），又设为特征空间（希尔伯特空间），如果存在⼀个从x到H的映射

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171907067.png" alt="image-20211116171907067" style="zoom:67%;" />

使得对所有x,z∊x，函数K(x,z)满足条件

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116171917907.png" alt="image-20211116171917907" style="zoom:67%;" />

则称K(x,z)为核函数，Ø(x)为映射函数

### 核技巧在支持向量机中的应用

在对偶问题的目标函数（7.37）中的内积xi ·xj 可以用核函数K(xi ，xj )＝Ø(xi)·Ø(xj)来代替，此时对偶问题的目标函数变成
$$
W(α)=\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}α_iα_jy_iy_jK(x_i,x_j)-\sum_{j=1}^{N}α_i
$$
同样，分类决策函数中的内积也可以用核函数代替，而分类决策函数式成为

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116173213231.png" alt="image-20211116173213231" style="zoom:80%;" />

### 正定核

本节叙述正定核的充要条件。通常所说的核函数就是正定核函数

K(x,z)为正定核函数的充要条件是对任意$x_i∊X$ ，i＝1,2,…,m，K(x,z)对应的Gram矩阵$K=[K(x_i,x_j)]_{m✖m}$是半正定阵

### 常用核函数

1. 多项式核函数，对应的支持向量机是一个p次多项式分类器

   <img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116173313569.png" alt="image-20211116173313569" style="zoom:80%;" />

2.  高斯核函数

   <img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116173330431.png" alt="image-20211116173330431" style="zoom:80%;" />

   

3. 字符串核函数

   核函数不仅可以定义在欧式空间上，还可以定义在离散数据的集合上。比如，字符串核是定义在字符串集合上的核函数。字符串核函数在文本分类、信息检索、生物信息学等方面都有应用。

### 非线性支持向量机学习算法

输⼊：训练数据集$T＝\{(x 1 ，y 1 ),(x 2 ，y 2 ),…,(x N ,y N )\}$，其中$x_i∊x＝R^n ，y_i ∊ ＝{-1,+1}，i＝1,2,…,N；$
输出：分类决策函数

1. 选取适当的核函数K(x,z)和适当的参数C，构造并求解最优化问题,，求得最优解$a^* ＝(a_1^* ,a_2^* ,…,a_N^* )^T $ 

   <img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116172026895.png" alt="image-20211116172026895" style="zoom:65%;" />

2. 选择a* 的⼀个正分量$0<a_j^*<C$，计算

   
   $$
   b^*=y_j-\sum_{i=1}^{N}α_i^*y_iK(x_i\cdot x_j)
   $$

3. 构造决策函数

   <img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116174209285.png" alt="image-20211116174209285" style="zoom:80%;" />

   当K(x,z)是正定核函数时，问题7.95〜7.97是凸⼆次规划问题，解是存在的

## 序列最小最优化算法（SMO）

本节讲述其中的序列最小最优化（SMO）算法，SMO算法要解如下凸⼆次规划的对偶问题：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116172116537.png" alt="image-20211116172116537" style="zoom:67%;" />

SMO算法是⼀种启发式算法，是将大优化问题分解成多个小优化问题来求解。目标是求出一系列α和b，从而求出权重w

**SMO算法包括两个部分**：求解两个变量⼆次规划的解析方法和选择变量的启发式方法

**SMO的工作原理：**

每次循环中选择两个α进行优化处理，一旦找到一对合适（两个α要符合一定的条件）的α，那么就增大其中一个减小另一个

### 两个变量二次规划的求解方法

假设选择的两个变量是a1 ,a2 ，其他变量ai (i＝3,4,…,N)是固定的。于是SMO的最优化问题7.98〜7.100的子问题可以写成：

<img src="images/7.%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/image-20211116172131744.png" alt="image-20211116172131744" style="zoom:67%;" />

## 线性支持向量机+简单SMO实现

1. 载入数据

   ```python
   def loadDataSet(fileName):
       dataMat = []
       labelMat = []
       fr = open(fileName)
       for line in fr.readlines():
           lineArr = line.strip().split('\t')
           dataMat.append([float(lineArr[0]), float(lineArr[1])])
           labelMat.append(float(lineArr[2]))
       return dataMat,labelMat
   ```

2. 随机选择第二个alpha

   ```python
   #i是第一个alpha的下标，m是所有alpha的数目
   def selectJrand(i,m):
       j=i 
       while (j==i):
           j = int(random.uniform(0,m))
       return j#进行随机选择
   ```

3. 调整alpha的值

   ```python
   #调整大于H或小于L的aj的值
   def clipAlpha(aj,H,L):
       if aj > H: 
           aj = H
       if L > aj:
           aj = L
       return aj
   ```

4. 求得最优alpha，b

   ```python
   #简化版SMO
   #输入：数据集，标签，常熟C，容错率，推出前最大循环次数
   #整个迭代更新过程都在将公式转化成代码语言
   def smoSimple(dataMatIn, classLabels, C, toler, maxIter):
       dataMatrix = mat(dataMatIn)
       labelMat = mat(classLabels).transpose()#列向量
       b = 0
       m,n = shape(dataMatrix)
       alphas = mat(zeros((m,1)))
       iter = 0
       while (iter < maxIter):
           alphaPairsChanged = 0
           for i in range(m):
               #multiply：数组和矩阵对应位置相乘，.T是转置操作,*是矩阵内积
               fXi = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b#y'=aiyixi x+b
               Ei = fXi - float(labelMat[i])#如果误差很大就进入优化过程
               if ((labelMat[i]*Ei < -toler) and (alphas[i] < C)) or ((labelMat[i]*Ei > toler) and (alphas[i] > 0)):
                   j = selectJrand(i,m)
                   fXj = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b
                   Ej = fXj - float(labelMat[j])#求第j个数据的误差
                   alphaIold = alphas[i].copy()
                   alphaJold = alphas[j].copy()
                   #书上208页二变量优化问题定义L,H取值如下
                   if (labelMat[i] != labelMat[j]):
                       L = max(0, alphas[j] - alphas[i])
                       H = min(C, C + alphas[j] - alphas[i])
                   else:
                       L = max(0, alphas[j] + alphas[i] - C)
                       H = min(C, alphas[j] + alphas[i])
                   if L==H: 
                       print ("L==H")
                       continue
                   #书209公式，eta=K11+K22-2K12
                   eta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T
                   if eta >= 0: 
                       print ("eta>=0")
                       continue
                   #更新alphaj
                   alphas[j] -= labelMat[j]*(Ei - Ej)/eta
                   alphas[j] = clipAlpha(alphas[j],H,L)
                   if (abs(alphas[j] - alphaJold) < 0.00001): 
                       print("j not moving enough")
                       continue
                   #还是公式，（7.109），更新alphai
                   alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])
                   #更新b，公式（7.115)
                   b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T
                   b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T
                   if (0 < alphas[i]) and (C > alphas[i]): 
                       b = b1
                   elif (0 < alphas[j]) and (C > alphas[j]):
                       b = b2
                   else:
                       b = (b1 + b2)/2.0
                   alphaPairsChanged += 1
                   print ("iter: %d i:%d, pairs changed %d" % (iter,i,alphaPairsChanged))
           if (alphaPairsChanged == 0): 
               iter += 1
           else: iter = 0
           print ("iteration number: %d" % iter)
       return b,alphas
   ```

## 线性支持向量机+核转换函数+完整SMO实现

1. 封装数据

   ```python
   class optStruct:
       def __init__(self,dataMatIn, classLabels, C, toler, kTup):  # Initialize the structure with the parameters 
           self.X = dataMatIn
           self.labelMat = classLabels
           self.C = C
           self.tol = toler
           self.m = shape(dataMatIn)[0]
           self.alphas = mat(zeros((self.m,1)))
           self.b = 0
           self.eCache = mat(zeros((self.m,2))) #first column is valid flag
           self.K = mat(zeros((self.m,self.m)))
           for i in range(self.m):
               self.K[:,i] = kernelTrans(self.X, self.X[i,:], kTup)
   ```

2. 计算误差

   ```python
     #计算误差   
   def calcEk(oS, k):
       fXk = float(multiply(oS.alphas,oS.labelMat).T*oS.K[:,k] + oS.b)
       Ek = fXk - float(oS.labelMat[k])
       return Ek
   ```

3. 选择第二个alpha

   ```python
     #选择第二个alpha，要步长最大
   def selectJ(i, oS, Ei):      
       maxK = -1
       maxDeltaE = 0
       Ej = 0
       oS.eCache[i] = [1,Ei]  #设为有效的，找到最大步长的j
       #nonzero返回非零元素的行列（分两个数组返回），.A是矩阵转数组
       validEcacheList = nonzero(oS.eCache[:,0].A)[0]
       if (len(validEcacheList)) > 1:
           for k in validEcacheList:   #循环列表找到最大的误差
               if k == i: continue #don't calc for i, waste of time
               Ek = calcEk(oS, k)
               deltaE = abs(Ei - Ek)
               #找到最大的补偿对应的k
               if (deltaE > maxDeltaE):
                   maxK = k
                   maxDeltaE = deltaE
                   Ej = Ek
           return maxK, Ej
       else:   #没有效的就随机找一个
           j = selectJrand(i, oS.m)
           Ej = calcEk(oS, j)
       return j, Ej
   ```

4. 更新记录误差

   ```python
   #更新记录误差
   def updateEk(oS, k):#after any alpha has changed update the new value in the cache
       Ek = calcEk(oS, k)
       oS.eCache[k] = [1,Ek]
   ```

5.  核转换函数（将低维的非线性数据转化成高维的线性数据）

   ```python
   def kernelTrans(X, A, kTup): #calc the kernel or transform data to a higher dimensional space
       m,n = shape(X)
       K = mat(zeros((m,1)))
       if kTup[0]=='lin': K = X * A.T   #linear kernel
       elif kTup[0]=='rbf':
           for j in range(m):
               deltaRow = X[j,:] - A
               K[j] = deltaRow*deltaRow.T
           K = exp(K/(-1*kTup[1]**2)) #divide in NumPy is element-wise not matrix like Matlab
       else: raise NameError('Houston We Have a Problem -- \
       That Kernel is not recognized')
       return K
   ```

6. 完整SMO优化过程，其实和SimSmo一样，只不过数据用的封装的

   ```python
   def innerL(i, oS):
       Ei = calcEk(oS, i)
       if ((oS.labelMat[i]*Ei < -oS.tol) and (oS.alphas[i] < oS.C)) or ((oS.labelMat[i]*Ei > oS.tol) and (oS.alphas[i] > 0)):
           j,Ej = selectJ(i, oS, Ei) #this has been changed from selectJrand
           alphaIold = oS.alphas[i].copy()
           alphaJold = oS.alphas[j].copy()
           if (oS.labelMat[i] != oS.labelMat[j]):
               L = max(0, oS.alphas[j] - oS.alphas[i])
               H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])
           else:
               L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)
               H = min(oS.C, oS.alphas[j] + oS.alphas[i])
           if L==H: 
               print( "L==H")
               return 0
           eta = 2.0 * oS.K[i,j] - oS.K[i,i] - oS.K[j,j] 
           if eta >= 0: 
               print("eta>=0")
               return 0
           oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta
           oS.alphas[j] = clipAlpha(oS.alphas[j],H,L)
           updateEk(oS, j) #添加到Ecache中
           if (abs(oS.alphas[j] - alphaJold) < 0.00001):
               print ("j not moving enough")
               return 0
           oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])#update i by the same amount as j
           updateEk(oS, i) #added this for the Ecache                    #the update is in the oppostie direction
           b1 = oS.b - Ei- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,i] - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[i,j]
           b2 = oS.b - Ej- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,j]- oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[j,j]
           if (0 < oS.alphas[i]) and (oS.C > oS.alphas[i]):
               oS.b = b1
           elif (0 < oS.alphas[j]) and (oS.C > oS.alphas[j]):
               oS.b = b2
           else: 
               oS.b = (b1 + b2)/2.0
           return 1
       else:
           return 0
   ```

7. 计算w

   ```python
   def calcWs(alphas,dataArr,classLabels):
       X = mat(dataArr); labelMat = mat(classLabels).transpose()
       m,n = shape(X)
       w = zeros((n,1))
       for i in range(m):
           w += multiply(alphas[i]*labelMat[i],X[i,:].T)
       return w
   ```

## 扩展SVM用于多类分类

[SVM用于多类分类](https://blog.csdn.net/qq_35551200/article/details/80295310?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EOPENSEARCH%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EOPENSEARCH%7Edefault-1.no_search_link)

