## 第三章 K邻近法

### 本章概要

1. k近邻法是⼀种基本**分类与回归**方法。分类时，给定⼀个训练数据集，对输⼊实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输⼊实例分为这个类。

2. k近邻法的三个基本要素：**k值的选择（常由交叉验证选择）、距离度量（常用欧氏距离，PL距离）及分类决策规则（常用多数表决）**

3. kd树是一种对k维空间的实例点进行存储，（**这里的k与k邻近法的k意义不同，表示向量的k维空间**）以便对其进行快速检索的树形结构。**kd树是二叉树**，构造kd树相当于不断的用垂直于坐标轴的超平面将k维空间进行划分，构成一系列的n维超矩阵区域。kd树的每个结点对应于一个k维超矩形区域。

4. kd树构造

   构造根结点，使根结点对应于k维空间中包含所有实例点的超矩形区域，通过下面的递归方法，不断地对k维空间进行切分，生成子结点。在超矩形区域（结点）上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域 （子结点）；这时，实例被分到两个子区域。这个过程直到子区域内没有实例时终止（终止时的结点为叶结点)。在此过程中，将实例保存在相应的结点上。

   通常，依次选择坐标轴对空间切分，选择训练实例点在选定坐标轴上的中位数 （median）为切分点，这样得到的**kd**树是平衡的。注意，平衡的kd树搜索时的效率未必是最优的。

5. kd树构造算法

   输入：k维空间数据集$T=\{x_1,x_2,……,x_N\}$，其中$x_i=(x^{(1)}_i,……x^{(2)}_i,x^{(k)}_i)$，i=1……N

   输出：kd树

   步骤如图所示：

   <img src="images/%E7%AC%AC%E4%B8%89%E7%AB%A0%20K%E9%82%BB%E8%BF%91%E6%B3%95/image-20211103212921854.png" alt="image-20211103212921854" style="zoom:80%;" />

6. kd树的搜索

   当我们生成kd树以后，就可以去预测测试集里面的样本目标点。预测的过程如下：

   1. 在kd树中找出包含目标点x的叶结点：从根结点出发，递归地向下访问kd树。若⽬标点x当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止。
   2. 以此叶结点为“当前最近点”
   3. 递归地向上回退，在每个结点进行以下操作：
      - 如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”
      - 当前最近点⼀定存在于该结点⼀个子结点对应的区域。检查该子结点的父结点的另⼀子结点对应的区域是否有更近的点。具体地，检查另⼀子结点对应的区域是否与以目标点为球心、以目标点与“当前最近点”间的距离为半径的超球体相交
   4. 回退到根结点时，搜索结束。最后的“当前最近点”即为x的最近邻点。

**例,课后题3.2，假设在上面构建的KD树中搜索（3，4.5）的最近邻点：**

1. 首先找到点x=(3,4.5)所在领域的叶节点$x_1=(4,7)^T$，则最近邻点一定在以x为圆心，x到$x_1$距离为半径的圆内；
2. 找到$x_1$的父节点$x_2=(5,4)^T$，$x_2$的另一子节点为$x_3=(2,3)^T$，此时$x_3$在圆内，故$x_3$为最新的最近邻点，并形成以x为圆心，以x到$x_3$距离为半径的圆；
3. 继续探索$x_2$的父节点$x_4=(7,2)^T$,$x_4$的另一个子节点(9,6)对应的区域不与圆相交，故不存在最近邻点，所以最近邻点为$x_3=(2,3)^T$。

可得到点$x=(3,4.5)^T$的最近邻点是(2,3)

### K邻近模型

#### 距离度量

特征空间中两个实例点的距离是两个实例点相似程度的反映。k近邻模型的特征空间⼀般是n维实数向量空间，通常使用的距离是欧氏距离，也可用Lp距离(p≥1)，即：
$$
L_p(x_i,x_j)=(\sum_{l=1}^{N}|{x}^{(l)}_{i}-{x}^{(l)}_{j}|^p)^\frac{1}{p}
$$
当p=2时，即为欧式距离，p=1为曼哈顿距离。由不同的距离度量所确定的最近邻点是不同的。下图为p取不同值时，与原点距离为1的点的图形

<img src="images/%E7%AC%AC%E4%B8%89%E7%AB%A0%20K%E9%82%BB%E8%BF%91%E6%B3%95/image-20211103175134292.png" alt="image-20211103175134292" style="zoom:50%;" />

#### k值的选择

k值的选择会对k近邻法的结果产生很大影响，不可以很小，也不可以很大

k值的过小就意味着整体模型变得复杂，容易发生过拟合；k值过大，这时与输⼊实例较远的（不相似的）训练实例也会对预测起作用，使预测错误。在应用中，k值⼀般取⼀个比较小的数值。通常采用交叉验证法来选取最优的k值

#### 分类决策规则

一般使用多数表决原则，如果损失函数为0-1损失函数，对给定实例x，其最近邻的k个训[统计学习方法习题解答](https://datawhalechina.github.io/statistical-learning-method-solutions-manual/)练实例点构成集合$N_k(x)$。如果涵盖$N_k(x)$的区域的类别是$c_j$，误分类率为

<img src="images/%E7%AC%AC%E4%B8%89%E7%AB%A0%20K%E9%82%BB%E8%BF%91%E6%B3%95/image-20211103175203942.png" alt="image-20211103175203942" style="zoom:70%;" />

要使误分类率最小即经验风险最小，就要使分类中$\sum I(y_i=c_i)$最大，所以多数表决规则等价于经验风险最小化。

### 算法实现

#### 原始实现：

```python
import numpy as np
from numpy import *   
import operator
import matplotlib.pyplot as plt
%matplotlib inline
```

原始算法即为线性扫描，这时要计算输⼊实例与每⼀个训练实例的距离，算法如下：

```python
'''inX是用于分类的输入向量，dataSet为输入的训练样本集，labels为训练样本集对应的标签，k表取邻近的数目'''
def classify0(inX,dataSet,labels,k):
    dataSetSize=dataSet.shape[0]
    #计算欧式距离
    diffMat=tile(inX,(dataSetSize,1))-dataSet#inX复制成和dataSet一样多行的矩阵
    sqDiffMat=diffMat**2
    sqDistances=sqDiffMat.sum(axis=1)#每一行相加
    distances=sqDistances**0.5#得到输入向量到训练样本集其它所有点的距离
    #对距离排序
    sortedDistIndicies=distances.argsort()
    #选取k个最近的点
    classCount={}#字典
    for i in range(k):
        voteIlabe=labels[sortedDistIndicies[i]]
        classCount[voteIlabe]=classCount.get(voteIlabe,0)+1#.get(voteIlabe,0):有voteIlabe对应值则返回，没有则为0
        
    #排序
    #operator.itemgetter(1)用法见百度，key整体功能为按字典（key，values）的values排序
    sortedClassCount=sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)
    return sortedClassCount[0][0]#返回预测的类别
```

例：使用K-临近算法改进约会网站的配对效果（来源：机器学习实战）

步骤：

1. 准备数据：解析文本文件，处理行，列，建立数据矩阵（1000\*3）和对应的标签向量（1000\*1）

   ```python
   '''打开文件'''
   filename='./KNNdatingTestSet2.txt'
   fr=open(filename)
   '''读取行数，列数'''
   filelines=fr.readlines()
   numberOfrow=len(filelines)
   first_line = filelines[0]
   numberOfcolumn=0
   for i in first_line:
       if(i=='\t'):
           count+=1
   '''创建数据矩阵dataMat和标签向量classLabelVector'''
   dataMat=np.zeros((numberOfrow,numberOfcolumn))
   index=0
   classLabelVector=[]
   for line in filelines:
       line=line.strip()
       listFromLine=line.split('\t')
       dataMat[index,:]=listFromLine[0:3]
       classLabelVector.append(listFromLine[3])
       index=index+1
   ```

2. 分析数据：使用Matplotlib创建散点图观察

   ```python
   '''分析数据，可视化数据点分布'''
   fig=plt.figure(figsize=(10,4))#画布大小10*4
   ax=fig.add_subplot(121)#1*1的网格放一个图，若为221，则2*2的网格四个图里的第一个
   ax2=fig.add_subplot(122)
   classLabelOfFloat= array(classLabelVector).astype(float)
   ax.scatter(dataMat[:,1],dataMat[:,2],15.0*classLabelOfFloat,15.0*classLabelOfFloat)#后两个参数对应于?怎么加图例?
   ax2.scatter(dataMat[:,0],dataMat[:,1],15.0*classLabelOfFloat,15.0*classLabelOfFloat)
   plt.show()
   ```

   <img src="images/%E7%AC%AC%E4%B8%89%E7%AB%A0%20K%E9%82%BB%E8%BF%91%E6%B3%95/image-20211103175239904.png" alt="image-20211103175239904" style="zoom:80%;" />

3. 准备数据：数据归一化（处理不同范围的特征值取值），这里使用一般的归一化方式
   $$
   newValue=\frac{oldValue-min}{max-min}
   $$

   ```python
   minVals=dataMat.min(0)#返回每一列的最小最大值
   maxVals=dataMat.max(0)
   normDataSet=np.zeros(shape(dataMat))
   m=dataMat.shape[0]#1000
   ranges=maxVals-minVals
   normDataSet=dataMat-tile(minVals,(m,1))#将原矩阵纵向地复制m个，原矩阵1*3，复制完1000*3
   normDataSet=normDataSet/tile(ranges,(m,1))
   ```

4. 测试算法：我们可以采用错误率检测分类器的性能

   ```python
   hoRatio=0.10#测试比例
   numTestVecs=int(m*hoRatio)
   errorCount=0.0
   for i in range(numTestVecs):
       classifierResult=classify0(normDataSet[i,:],normDataSet[numTestVecs:m,:],classLabelVector[numTestVecs:m],3)
       print("the classifier came back with: %d,the real answer is: %d" % (int(classifierResult),classLabelOfFloat[i]))
       if(classifierResult!=classLabelVector[i]):
           errorCount+=1
   print("the classifier error rate is: %f"%(errorCount/float(numTestVecs)))
   ```

#### kd树实现

使用特殊的结构存储训练数据，以减少计算距离的次数。本算法实现见示范， 思想就是那么个思想，实现起来还是有点复杂，以后再自己实现

[KDTree代码](https://github.com/SmallVagetable/machine_learning_python/blob/master/knn/knn_kdtree.py)

