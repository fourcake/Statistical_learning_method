# 第十章 隐马尔科夫模型

## 本章概要

1. 隐马尔可夫模型是关于时序的概率模型，描述由⼀个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成⼀个观测而产生观测随机序列的过程

2. 隐马尔可夫链随机生成的状态的序列，称为状态序列，每个状态生成⼀个观测，而由此产生的观测的随机序列，称为观测序列。序列的每⼀个位置可以看作是⼀个时刻

3. 设Q是所有可能的状态的集合，V是所有可能的观测的集合，其中N是可能的状态数，M是可能的观测数
   $$
   Q=\{q_1,q_2,\cdots,q_N\},V=\{v_1,v_2,\cdots,v_M\}
   $$
   I是长度为T的状态序列，O是对应的观测序列
   $$
   I=(i_1,i_2,\cdots,i_T),O=(o_1,o_2,\cdots,o_T)
   $$
   A是状态转移概率矩阵$A=\{a_{ij}\}_{N* N}$，其中，aij是在时刻t处于状态qi的条件下在时刻t+1转移到状态qj的概率
   $$
   a_{ij}=P(i_{t+1}=q_j|i_t=q_i),i=1,2,\cdots,N,j=1,2,\cdots,N
   $$
   B是观测概率矩阵$B=[b_j(k)]_{N*M}$，其中，bj是在时刻t处于状态qj的条件下生成观测vk的概率
   $$
   b_j(k)=P(o_t=v_k|i_t=q_j),k=1,2,\cdots,M,j=1,2,\cdots,N
   $$
   $\pi=(\pi_i)$是初始概率向量，其中$\pi_i=P(i_1=q_i),i=1,2,\cdots,N$是时刻听处于状态qi的概率

4. 隐马尔可夫模型由初始状态概率向量$\pi$ 、状态转移概率矩阵A和观测概率矩阵B决定。$\pi$和A决定状态序列，B决定观测序列

5. 隐马尔可夫模型两个基本假设

   1. 齐次马尔可夫性假设，即假设隐藏的马尔可夫链在任意时刻t的状态只依赖于其前⼀时刻的状态，与其他时刻的状态及观测无关，
      与时刻t无关。
   2. 观测独立性假设，即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关

## 观测序列生成过程

输入：隐马尔可夫模型$\lambda=(A,B,\pi)$，观测序列长度T
输出：观测序列$O=(o_1,o_2,\cdots,o_T)$

1. 按照初始状态分布产生状态$i_1$
2. 令t=1
3. 按照状态$i_t$的观测概率分布$b_it(k)$生成$o_t$
4. 按照状态$i_t$的状态转移概率分布$\{a_{i_ti_{t+1}}\}$产生状态$i_{t+1}$
5. 令t＝t+1；如果t<T，转步（3）；否则，结束

## 隐马尔可夫模型三个基本问题

### 概率计算问题

给定模型$\lambda=(A,B,\pi)$和观测序列$,O=(o_1,o_2,\cdots,o_T)$，计算在该模型下观测序列O出现的概率$P(O|\lambda)$

#### 前向后向算法

**前向概率：**给定隐马尔可夫模型$\lambda$，定义到时刻t部分观测序列为$o_1,o_2,\cdots,o_t$ 且状态为qi的概率为前向概率，记作
$$
a_t(i)=P(o_1,o_2,\cdots,o_t,i_t=q_i|\lambda)
$$
可以递推地求得前向概率at(i)及观测序列概率$P(O|\lambda)$。

**后向概率：**给定隐马尔可夫模型$\lambda$，定义在时刻t状态为qi的条件下，从t+1到T的部分观测序列为$o_{t+1},o_{t+2}，\cdots,o_T$  的概率为后向概率，记作
$$
\beta_t(i)=P(o_{t+1},o_{t+2}，\cdots,o_T|i_t=q_i,\lambda)
$$
可以递推地求得后向概率$\beta_t(i)$及观测序列概率$P(O|\lambda)$。

**观测序列概率的前向算法:**

输入：隐马尔可夫模型$\lambda$，观测序列O
输出：观测序列概率$P(O|\lambda)$

1. 设置初值
   $$
   a_1(i)=\pi_ib_i(o_1),i=1,2,\cdots,N
   $$

2. 递推，对t=1，2，……，T-1
   $$
   a_{t+1}(i)=[\sum_{j=1}^{N}a_t(j)a_{ji}]b_i(o_{t+1}),i=1,2,\cdots,N
   $$

3. 终止
   $$
   P(O|\lambda)=\sum_{i=1}^{N}a_T(i)
   $$

例题：

![image-20211124150403013](images/10.%20%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/image-20211124150403013.png)

![image-20211124150416882](images/10.%20%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/image-20211124150416882.png)

**观测序列概率的后向算法：**

输入：隐马尔可夫模型$\lambda$，观测序列O
输出：观测序列概率$P(O|\lambda)$

1. 设置初值
   $$
   \beta_T(i)=1,i=1,2,\cdots,N
   $$

2. 对t=T-1,T-2,……，1
   $$
   \beta_t(i)=\sum_{j=1}^{N}a_{ij}b_j(o_{t+1})\beta_{t+1}(j),i=1,2,\cdots,N
   $$

3. $$
   P(O|\lambda)=\sum_{i=1}^{N}\pi_ib_i(o_1)\beta_1(i)
   $$

   

将前向后向算法合并可得到下式，此式当t＝1和t＝T-1时分别为式前向概率和式后向概率
$$
P(O|\lambda)=\sum_{i=1}^{N}\sum_{j=1}^{N}a_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)
$$

### 学习问题

已知观测序列$o_1,o_2,\cdots,o_t$估计模型$\lambda=(A,B,\pi)$的参数，使得在该模型下观测序列概率$P(O|\lambda)$最d大。即用极大似然估计的方法估计参数

#### 监督学习（训练数据是包括观测序列和对应的状态序列）

假设已给训练数据包含S个长度相同的观测序列和对应的状态序列${(O_1 ,I_1 ),(O_2 ,I_ 2 ),…,(O_ S ,I_ S )}$，那么可以利⽤极⼤似然估计法来估计隐马尔可夫模型的参数

1. 转移概率$a_{ij}$的估计(样本中时刻t处于状态i时刻t+1转移到状态j的频数为Aij)
   $$
   \hat{a_{ij}}=\frac{A_{ij}}{\sum_{j=1}^{N}A_{ij}};i,j=1,2,\cdots,N
   $$

2. 观测概率$b_j(k)$的估计(样本中状态为j并观测为k的频数是Bjk)
   $$
   \hat{b}_j(k)=\frac{B_{jk}}{\sum_{k=1}^{M}B_{jk}};j=1,2,\cdots,N;k=1,2,\cdots,M
   $$

3. 初始状态概率$\pi_i$的估计为S个样本中初始状态为qi的频率，由于监督学习需要使用训练数据，而人工标注训练数据往往代价很
   高，有时就会利用非监督学习的方法

#### 非监督学习（训练数据是包括观测序列）

我们将观测序列数据看作观测数据O，状态序列数据看作不可观测的隐数据I，那么隐马尔可夫模型事实上是⼀个含有隐变量的概率模型
$$
P(O|\lambda)=\sum_tP(O|I,\lambda)P(I|\lambda)
$$
它的参数学习可以由EM算法实现
$$
a_{ij}=\frac{\sum_{t=1}^{T-1}\xi_t(i,j)}{\sum_{t=1}^{T-1}\gamma_t(i)}
$$

$$
b_j(k)=\frac{\sum_{t=1,o_t=v_k}^{T}\gamma_t(j)}{\sum_{t=1}^{T}\gamma_t(j)}
$$

$$
\pi_i=\gamma_1(i)
$$

其中
$$
\gamma_t(i)=\frac{a_t(i)\beta_t(i)}{P(O|\lambda)}=\frac{a_t(i)\beta_t(i)}{\sum_{j=1}^{N}a_t(j)\beta_t(j)}
$$

$$
\xi_t(i,j)=\frac{a_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^{N}\sum_{j=1}^{N}a_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}
$$



### 预测问题

已知模型$\lambda=(A,B,\pi)$和观测序列$o_1,o_2,\cdots,o_t$，求对给定观测序列条件概率P(I|O)最大的状态序列$I＝(i_1 ，i_2 ,…,i_T )$。即给定观测序列，求最有可能的对应的状态序列

#### 近似算法

近似算法的想法是，在每个时刻t选择在该时刻最有可能出现的状态，从而得到⼀个状态序列，将它作为预测的结果。
给定隐马尔可夫模型$\lambda$观测序列O，在时刻t处于状态qi的概率$\gamma_t(i)$，则概率最大的i就是预测状态

#### 维特比算法

用动态规划求概率最大路径

定义在时刻t状态为i的所有单个路径（i1,i2,……,it）中概率最大的为
$$
\delta_t(i)=\max_{i_1,i_2,\cdots,i_{t-1}}P(i_t=i,i_{t-1},\cdots,i_1,o_t,\cdots,o_1|\lambda);i=1,2,\cdots,N
$$

$$
\delta_{t+1}(i)=\max_{1≤j≤N}[\delta_t(j)a_{ji}]b_i(o_{t+1})
$$

定义在时刻t状态为i的所有单个路径$(i_1,i_2,\cdots,i_{t-1},i)$中概率最大的路径的第t-1个结点为
$$
\psi_t(i)=\arg\max_{1≤j≤N}[\delta_{t-1}(j)a_{ji}]
$$
输入：隐马尔可夫模型$\lambda$，观测序列O
输出：最优路径$I=(i_1,i_2,\cdots,i_T)$

1. 初始化
   $$
   \delta_1(i)=\pi_ib_i(o_1),i=1,2,\cdots,N
   $$

   $$
   \psi_1(i)=0,i=1,2,\cdots,N
   $$

2. 递推，对，t=2，3，……，T
   $$
   \delta_t(i)=\max_{1≤j≤N}[\delta_{t-1}(j)a_{ji}]b_i(o_{t})
   $$

   $$
   \psi_t(i)=\arg\max_{1≤j≤N}[\delta_{t-1}(j)a_{ji}]
   $$

3. 终止

   <img src="images/10.%20%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/image-20211124150443233.png" alt="image-20211124150443233" style="zoom:80%;" />

举例

![image-20211124150505027](images/10.%20%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/image-20211124150505027.png)

![image-20211124150515979](images/10.%20%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/image-20211124150515979.png)

![image-20211124150528697](images/10.%20%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/image-20211124150528697.png)

## 代码实现

```python
class HiddenMarkov:
    def __init__(self):
        self.alphas = None
        self.forward_P = None
        self.betas = None
        self.backward_P = None

    # 前向算法
    def forward(self, Q, V, A, B, O, PI):
        # 状态序列的大小
        N = len(Q)
        # 观测序列的大小
        M = len(O)
        # 初始化前向概率alpha值
        alphas = np.zeros((N, M))
        # 时刻数=观测序列数
        T = M
        # 遍历每一个时刻，计算前向概率alpha值
        for t in range(T):
            # 得到序列对应的索引
            indexOfO = V.index(O[t])
            # 遍历状态序列
            for i in range(N):
                # 初始化alpha初值
                if t == 0:
                    # P176 公式(10.15)
                    alphas[i][t] = PI[t][i] * B[i][indexOfO]
                    print('alpha1(%d) = p%db%db(o1) = %f' %
                          (i + 1, i, i, alphas[i][t]))
                else:
                    # P176 公式(10.16)
                    alphas[i][t] = np.dot([alpha[t - 1] for alpha in alphas],
                                          [a[i] for a in A]) * B[i][indexOfO]
                    print('alpha%d(%d) = [sigma alpha%d(i)ai%d]b%d(o%d) = %f' %
                          (t + 1, i + 1, t - 1, i, i, t, alphas[i][t]))
        # P176 公式(10.17)
        self.forward_P = np.sum([alpha[M - 1] for alpha in alphas])
        self.alphas = alphas
    # 后向算法
    def backward(self, Q, V, A, B, O, PI):
        # 状态序列的大小
        N = len(Q)
        # 观测序列的大小
        M = len(O)
        # 初始化后向概率beta值，P178 公式(10.19)
        betas = np.ones((N, M))
        #
        for i in range(N):
            print('beta%d(%d) = 1' % (M, i + 1))
        # 对观测序列逆向遍历
        for t in range(M - 2, -1, -1):
            # 得到序列对应的索引
            indexOfO = V.index(O[t + 1])
            # 遍历状态序列
            for i in range(N):
                # P178 公式(10.20)
                betas[i][t] = np.dot(
                    np.multiply(A[i], [b[indexOfO] for b in B]),
                    [beta[t + 1] for beta in betas])
                realT = t + 1
                realI = i + 1
                print('beta%d(%d) = sigma[a%djbj(o%d)beta%d(j)] = (' %
                      (realT, realI, realI, realT + 1, realT + 1),
                      end='')
                for j in range(N):
                    print("%.2f * %.2f * %.2f + " %
                          (A[i][j], B[j][indexOfO], betas[j][t + 1]),
                          end='')
                print("0) = %.3f" % betas[i][t])
        # 取出第一个值
        indexOfO = V.index(O[0])
        self.betas = betas
        # P178 公式(10.21)
        P = np.dot(np.multiply(PI, [b[indexOfO] for b in B]),
                   [beta[0] for beta in betas])
        self.backward_P = P
        print("P(O|lambda) = ", end="")
        for i in range(N):
            print("%.1f * %.1f * %.5f + " %
                  (PI[0][i], B[i][indexOfO], betas[i][0]),
                  end="")
        print("0 = %f" % P)
 # 维特比算法
    def viterbi(self, Q, V, A, B, O, PI):
        # 状态序列的大小
        N = len(Q)
        # 观测序列的大小
        M = len(O)
        # 初始化daltas
        deltas = np.zeros((N, M))
        # 初始化psis
        psis = np.zeros((N, M))
        # 初始化最优路径矩阵，该矩阵维度与观测序列维度相同
        I = np.zeros((1, M))
        # 遍历观测序列
        for t in range(M):
            # 递推从t=2开始
            realT = t + 1
            # 得到序列对应的索引
            indexOfO = V.index(O[t])
            for i in range(N):
                realI = i + 1
                if t == 0:
                    # P185 算法10.5 步骤(1)
                    deltas[i][t] = PI[0][i] * B[i][indexOfO]
                    psis[i][t] = 0
                    print('delta1(%d) = pi%d * b%d(o1) = %.2f * %.2f = %.2f' %
                          (realI, realI, realI, PI[0][i], B[i][indexOfO],
                           deltas[i][t]))
                    print('psis1(%d) = 0' % (realI))
                else:
                    # # P185 算法10.5 步骤(2)
                    deltas[i][t] = np.max(
                        np.multiply([delta[t - 1] for delta in deltas],
                                    [a[i] for a in A])) * B[i][indexOfO]
                    print(
                        'delta%d(%d) = max[delta%d(j)aj%d]b%d(o%d) = %.2f * %.2f = %.5f'
                        % (realT, realI, realT - 1, realI, realI, realT,
                           np.max(
                               np.multiply([delta[t - 1] for delta in deltas],
                                           [a[i] for a in A])), B[i][indexOfO],
                           deltas[i][t]))
                    psis[i][t] = np.argmax(
                        np.multiply([delta[t - 1] for delta in deltas],
                                    [a[i] for a in A]))
                    print('psis%d(%d) = argmax[delta%d(j)aj%d] = %d' %
                          (realT, realI, realT - 1, realI, psis[i][t]))
        #print(deltas)
        #print(psis)
        # 得到最优路径的终结点
        I[0][M - 1] = np.argmax([delta[M - 1] for delta in deltas])
        print('i%d = argmax[deltaT(i)] = %d' % (M, I[0][M - 1] + 1))
        # 递归由后向前得到其他结点
        for t in range(M - 2, -1, -1):
            I[0][t] = psis[int(I[0][t + 1])][t + 1]
            print('i%d = psis%d(i%d) = %d' %
                  (t + 1, t + 2, t + 2, I[0][t] + 1))
        # 输出最优路径
        print('最优路径是：', "->".join([str(int(i + 1)) for i in I[0]]))     
```

示例

```python
Q = [1, 2, 3]
V = ['红', '白']
A = [[0.5, 0.2, 0.3], [0.3, 0.5, 0.2], [0.2, 0.3, 0.5]]
B = [[0.5, 0.5], [0.4, 0.6], [0.7, 0.3]]
# O = ['红', '白', '红', '红', '白', '红', '白', '白']
O = ['红', '白', '红', '白']    #习题10.1的例子
PI = [[0.2, 0.4, 0.4]]
HMM = HiddenMarkov()
# HMM.forward(Q, V, A, B, O, PI)
# HMM.backward(Q, V, A, B, O, PI)
HMM.viterbi(Q, V, A, B, O, PI)
```

[代码来源](https://github.com/fengdu78/lihang-code/blob/master/%E7%AC%AC10%E7%AB%A0%20%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/10.HMM.ipynb)