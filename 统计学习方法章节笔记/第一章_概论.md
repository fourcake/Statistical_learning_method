# 第一章  统计学习方法概论

## 本章概要

1. 统计学习包括监督学习、非监督学习、半监督学习和强化学习
2. 统计学习方法三要素——**模型(模型空间)、策略（损失函数：0-1，平方，对数等和风险函数：经验风险，结构风险）、算法**
3. 统计学习中，进行模型选择或者说提高学习的泛化能力是一个重要问题。如果只考虑减少训练误差，就可能产生**过拟合**现象。模型选择的方法有**正则化（对损失函数加惩罚）与交叉验证（简单交叉，S折交叉，留一交叉）**
4. 监督学习方法又可以分为生成方法和判别方法，所学到的模型分别称为**生成模型**（学习联合概率分布P(X,Y)：朴素贝叶斯，HMM）和**判别模型**（直接学习决策函数f(x)或条件概率分布：K领近，感知机，决策树等）
5. 监督学习问题：**分类，标注，回归**（最小二乘法）

## 关于最小二乘法拟和曲线

一般的![$H(x)$](https://render.githubusercontent.com/render/math?math=H%28x%29&mode=inline)为![$n$](https://render.githubusercontent.com/render/math?math=n&mode=inline)次的多项式$H(x)=w_0+w_1x+w_2x^2+...w_nx^n$$，$$w(w_0,w_1,w_2,...,w_n)$为参数，最小二乘法就是要找到一组参数$$w$$使得$\sum_{i=1}^n{(h(x_i)-y_i)^2}$(残差平方和) 最小。

例：我们用目标函数$y=sin2{\pi}x$ 加上一个正态分布的噪音干扰，用多项式去拟合例1.1:

```python
import numpy as np
import scipy as sp
from scipy.optimize import leastsq
import matplotlib.pyplot as plt
%matplotlib inline
```

$numpy.poly1d([1,2,3])$ 生成 $1x^2+2x^1+3x^0$，下面三个函数是最小二乘法代码实现的核心

```python
# 目标函数
def real_func(x):
    return np.sin(2*np.pi*x)

# 多项式
def fit_func(p, x):
    f = np.poly1d(p)
    return f(x)

# 残差
def residuals_func(p, x, y):
    ret = fit_func(p, x) - y
    return ret
```

$leastsq$函数参数第一项为误差函数，第二项为初始k，b这一类参数，第三项为数据点，$plt.legend()$使用默认参数label创建图例

```python
# 十个点,linspace(0,1,10)返回0到1之间十个均匀分布的数
x = np.linspace(0, 1, 10)
x_points = np.linspace(0, 1, 1000)
# 加上正态分布噪音的目标函数的值（y_+np.random.normal(0,0.1)）
y_ = real_func(x)
y = [np.random.normal(0, 0.1) + y1 for y1 in y_]

def fitting(M=0):
    """
    M    为 多项式的次数
    """
    # 随机初始化多项式参数
    p_init = np.random.rand(M + 1)
    # 最小二乘法,直接用Scipy库中的最小二乘函数 
    p_lsq = leastsq(residuals_func, p_init, args=(x, y))
    print('Fitting Parameters:', p_lsq[0])

    # 可视化
    plt.plot(x_points, real_func(x_points), label='real')
    plt.plot(x_points, fit_func(p_lsq[0], x_points), label='fitted curve')
    plt.plot(x, y, 'bo', label='noise')
    plt.legend()
    return p_lsq
```

M=0（p_lsq_0= fitting(M=0)）

<img src="images/%E7%AC%AC%E4%B8%80%E7%AB%A0_%E6%A6%82%E8%AE%BA/image-20211102174040299.png" alt="image-20211102174040299" style="zoom:50%;" />

M=5（p_lsq_5= fitting(M=5)）

<img src="images/%E7%AC%AC%E4%B8%80%E7%AB%A0_%E6%A6%82%E8%AE%BA/image-20211102174057828.png" alt="image-20211102174057828" style="zoom:50%;" />

M=9（p_lsq_9= fitting(M=9)）

<img src="images/%E7%AC%AC%E4%B8%80%E7%AB%A0_%E6%A6%82%E8%AE%BA/image-20211102174122076.png" alt="image-20211102174122076" style="zoom:50%;" />

当M=9时，发生过拟合，引入正则化项来降低过拟合，$Q(x)=\sum_{i=1}^n{(h(x_i)-y_i)^2}+\lambda||w||^2$，回归问题中，损失函数是平方损失，正则化可以是参数向量的L2范数,也可以是L1范数。

- L1: regularization*abs(p)
- L2: 0.5 * regularization * np.square(p)

```python
regularization = 0.0001
def residuals_func_regularization(p, x, y):
    ret = fit_func(p, x) - y                                           #yhat-y
    ret = np.append(ret,np.sqrt(0.5 * regularization * np.square(p)))  # L2范数作为正则化项
    return ret
```



```python
# 最小二乘法,加正则化项
p_init = np.random.rand(9 + 1)
p_lsq_regularization = leastsq(residuals_func_regularization, p_init, args=(x, y))
```

$p\_lsq\_regularization[0]$第0维存放训练好的多项式系数，第一维存放啥不知道

```python
plt.plot(x_points, real_func(x_points), label='real')
plt.plot(x_points, fit_func(p_lsq_9[0], x_points), label='fitted curve')
plt.plot(
    x_points,
    fit_func(p_lsq_regularization[0], x_points),
    label='regularization')
plt.plot(x, y, 'bo', label='noise')
plt.legend()
```

<img src="images/%E7%AC%AC%E4%B8%80%E7%AB%A0_%E6%A6%82%E8%AE%BA/image-20211102174151198.png" alt="image-20211102174151198" style="zoom:50%;" />


## 课后习题

1.1说明伯努利模型的极大似然估计以及贝叶斯估计中的统计学习方法三要素。伯努利模型是定义在取值为0与1的随机变量上的概率分布。假设观测到伯努利模型![$n$](https://render.githubusercontent.com/render/math?math=n&mode=inline)次独立的数据生成结果，其中$k$次的结果为1，这时可以用极大似然估计或贝叶斯估计来估计结果为1的概率。

1. **极大似然估计**：模型为条件概率分布，损失函数是对数函数时的经验风险最小化(ERM)
   **模型：** $\mathcal{F}=\{f|f_p(x)=p^x(1-p)^{(1-x)}\}$
   **策略：** 最大化似然函数
   **算法：** $\displaystyle \mathop{\arg\min}_{p} L(p)= \mathop{\arg\min}_{p} \binom{n}{k}p^k(1-p)^{(n-k)}$

2. **贝叶斯估计**：模型是条件概率分布，损失函数是对数损失函数，模型的复杂度由模型的先验概率展示时，结构风险最小化(SRM)等于贝叶斯估计中的最大后验概率（MAP）
   **模型：** $\mathcal{F}=\{f|f_p(x)=p^x(1-p)^{(1-x)}\}$
   **策略：** 最大后验概率$P(\theta)$为先验概率，中间式子就是贝叶斯公式，分母是固定值，使用期望求解方法见参考链接2
   **算法：**$\displaystyle \mathop{\arg\max}_{\theta} P(\theta|D)= \mathop{\arg\max}_{\theta} \frac{P(D|\theta)P(\theta)}{\int{P(D|\theta)P(\theta)d\theta}}=\displaystyle \mathop{\arg\max}_{\theta}P(D|\theta)P(\theta)$

   <img src="images/%E7%AC%AC%E4%B8%80%E7%AB%A0_%E6%A6%82%E8%AE%BA/image-20211102174231722.png" alt="image-20211102174231722" style="zoom:70%;" />

1.2通过经验风险最小化推导极大似然估计。证明模型是条件概率分布，当损失函数是对数损失函数时，经验风险最小化等价于极大似然估计。

$\ln L(\theta) = \sum_D \ln P_{\theta}(Y|X) \\ \mathop{\arg \max}_{\theta} \sum_D \ln P_{\theta}(Y|X) = \mathop{\arg \min}_{\theta} \sum_D (- \ln P_{\theta}(Y|X))$

即经验风险最小化等价于极大似然估计，亦可通过经验风险最小化推导极大似然估计。

## 参考文献

1. [极大似然估计和贝叶斯估计](https://zhuanlan.zhihu.com/p/61593112)
2. [统计学习方法习题解答](https://datawhalechina.github.io/statistical-learning-method-solutions-manual/#/chapter01/chapter01?id=%e4%b9%a0%e9%a2%9812)
3. [lihang-code](https://github.com/fengdu78/lihang-code/blob/master/%E7%AC%AC01%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA/1.Introduction_to_statistical_learning_methods.ipynb)

