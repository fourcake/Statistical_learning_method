# 第四章  朴素贝叶斯

## 本章概要

1. 首先分清概念：生成式和判别式

   - 判别式就是直接学习特征输入X和Y之间的关系，如决策函数Y=f(x)，或者从概率论角度，求出条件分布P(Y|X)。代表算法有**决策树、KNN、逻辑回归、支持向量机、随机条件场CRF等**
   - 生成式就是直接找出特征输出Y和特征X的联合分布P(X,Y)，然后用$P(Y|X)=\frac{P(X,Y)}{P(X)}$得出。代表算法有**朴素贝叶斯，隐马尔可夫**。朴素贝叶斯就是通过数据**学习P（X|Y）：（条件概率分布）和P（Y）：（先验概率分布）的估计，来得到联合概率分布**。

2. 朴素贝叶斯算法基于**贝叶斯定理和特征条件独立假设**，

   - 贝叶斯定理

     $P(class|data) = (P(data|class) * P(class)) / P(data)$

   - 特征条件独立假设：

     原本朴素贝叶斯要学习$P（Y=c_k）$和$P(X=x|Y=c_k)=P(X^{1}=x^{1},……,X^{n}=x^{n}|Y=c_k),k=1,2,……,K$，**特征条件独立假设$X$的$n$个特征在类确定的条件下都是条件独立的，即**
     $$
     P(X=x|Y=c_k)=\prod_{j=1}^{n} P(X^{j}=x^{j}|Y=c_k),k=1,2,……,K
     $$
     如此大大简化了计算过程，但是因为这个假设太过严格，所以会相应牺牲一定的准确率，因此称为**朴素**。

## 朴素贝叶斯算法流程

输入：m个样本，每个样本有n维，$T=(x_1,y_1),(x_2,y_2),……,(x_m,y_m)$，共有K个输出特征类别$y$从${c_!.c_2,……,c_k}$中取值

输出：为输入$x_{test}$的类别$y_{test}$

算法流程：

1. 首先计算Y的K个先验概率$P(Y=c_k)$：即类别为$c_k$的数/总数，也可以从数据中估计给定类别的条件概率

   对于分类变量，例如计数或标签，可以使用多项式分布。如果变量是二元的，例如是/否或真/假，则可以使用二项分布。如果变量是数字变量，例如测量值，通常使用高斯分布。

   **二元：二项分布（伯努利朴素贝叶斯）**

   **分类：多项分布（多项式朴素贝叶斯）**

   **数字：高斯分布（高斯贝叶斯）**

2. 计算条件概率分布$P(X=x|Y=c_k)=\prod_{j=1}^{n} P(X^{j}=x^{j}|Y=c_k),k=1,2,……,K$

3. 根据贝叶斯原理，计算后验概率
   $$
   P(Y=c_k|X=x)=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_k{P(X=x|Y=c_k)P(Y=c_k)}}
   $$
   代入应用特征条件独立假设后的式子可得
   $$
   P(Y=c_k|X=x)=\frac{P(Y=c_k)\prod_{j=1}^{n} P(X^{j}=x^{j}|Y=c_k)}{\sum_kP(Y=c_k ){\prod_{j=1}^{n} P(X^{j}=x^{j}|Y=c_k)}},k=1,2,……，K
   $$
   由于对所有的类别k，分母都相同，所以上式还可变为
   $$
   P(Y=c_k|X=x)={P(Y=c_k)\prod_{j=1}^{n} P(X^{j}=x^{j}|Y=c_k)},k=1,2,……，K
   $$

4. 计算$X_{test}$的类别，朴素贝叶斯分类器可以表示为
   $$
   y_{test}=f(x)=arg\max_{c_k} P(Y=c_k) \prod_{j=1}^{n}P(X^{(j)}=x^{(j)}_{test}|Y=c_k)
   $$

## 朴素贝叶斯的参数估计

朴素贝叶斯中，学习意味着要估计$P(Y=c_k)$和$P(X^{(j)}=x^{(j)}|Y=c_k)$，可以应用极大似然估计法估计相应的概率。先验概率$P(Y=c_k)$的极大似然估计是：(证明见课后习题4.1)
$$
P(Y=c_k)=\frac{\sum_{i=1}^{N}I(y_i=c_k)}{N},k=1,2,……,K
$$
设第j个特征$x^{(j)}$的可能取值集合为$\{a_{j1},a_{j2},……,a_{jsi}\}$，条件概率$P(X^{(j)}=x^{(j)}|Y=c_k)$的极大似然估计是：（证明见课后题）
$$
P(X^{(j)}=a_{(jl)}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x^{(j)}_i=a_{jl},y_i=c_k)}{\sum_{i=1}^{N}I(y_i=c_k)},j=1,2,……,n;l=1,2,……,S_i;k=1,2,……,K
$$
其实两个都是最原始的**符合条件样本数/样本总数**

**这种实现的示例在统计学习方法课本上有例子（极大似然估计和贝叶斯估计），较为简单，不过多介绍，可看课本理解原理，虽简单但是必须看！！！！！**

## 朴素贝叶斯(伯努利分布)--使用python进行文本分类（侮辱类与非侮辱类）

1. 准备数据：从文本中构建词向量，我们需要建立词表到向量的转换函数，**即一组单词转换成一组特征向量**

   ```python
   #创建一些实验样本(postingList本应该处理样本句子生成，这里省去这一步),即狗狗网站的六条评论
   #返回值postingList是进行词条切分后的文档集合，classVec是人工标注的类别标签集合
   def loadDataSet():
       postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],
                    ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
                    ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
                    ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
                    ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
                    ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
       classVec = [0,1,0,1,0,1]    #人工标注 1 is abusive, 0 not
       return postingList,classVec
   
   #创建一个包含在输入中出现的所有不重复词的列表，即由六条评论所有不重复词汇组成一个词汇表
   def createVocabList(dataSet):
       vocabSet = set([])  #set集，不允许重复
       for document in dataSet:
           vocabSet = vocabSet | set(document) #|为并操作
       return list(vocabSet)
   #输入为词汇表及某个文档，输出为文档向量，向量的每一元素为0或1，表示词汇表中的单词是否在输入文档中出现
   def setOfWords2Vec(vocabList, inputSet):
       returnVec = [0]*len(vocabList)#创建一个词汇表的全零向量
       for word in inputSet:
           if word in vocabList:
               returnVec[vocabList.index(word)] = 1
           else: print("the word: %s is not in my Vocabulary!" % word)
       return returnVec
   ```

2. 训练算法：从词向量计算概率，**即利用转换的特征向量计算概率**

   改写贝叶斯公式，应用到这个例子中，$\vec w$代表输入向量
   $$
   p(c_i|\vec{w})=\frac{p(\vec w|c_i)p(c_i)}{p(\vec w)}
   $$
   首先计算$p(c_i)$，即类别i中的文档书/总文档数

   再计算$p(\vec w|c_i)$，运用特征条件独立假设，$p(\vec w|c_i)=p(w_0|c_i)p(w_1|c_i)……p(w_n|c_i)$

   ```python
   #输入为文档矩阵（由所有文档的特征向量组成）和每一条文档对应的类别
   def trainNB0(trainMatrix,trainCategory):
       numTrainDocs = len(trainMatrix)
       numWords = len(trainMatrix[0])
       #计算p(1)
       pAbusive = sum(trainCategory)/float(numTrainDocs)#(1+1+1)/6
       #计算p(wi|c0)和p(wi|c1)
       p0Num = ones(numWords)
       p1Num = ones(numWords)#初始化俩条件概率的分子      
   #   p0Denom = 2.0
   #    p1Denom = 2.0#初始化俩概率的分母       
       for i in range(numTrainDocs):
           if trainCategory[i] == 1:#在类别为1时
               p1Num += trainMatrix[i]#特征向量每一维（每一属性）出现了几次，因为这题只有0，1，所以可以直接相加
             #  p1Denom += sum(trainMatrix[i])感觉不对，应该除以这个类别一共有多少
           else:
               p0Num += trainMatrix[i]
             #  p0Denom += sum(trainMatrix[i])
               
       p1Vect =log(p1Num/4)         #为在类别为1的条件下，每一维取1的取对数概率是多少
       p0Vect =log(p0Num/4)          
       return p0Vect,p1Vect,pAbusive
   ```

3. 预测概率

   ```python
   def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):
       p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #element-wise mult同位元素相乘
       p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)
       if p1 > p0:
           return 1
       else: 
           return 0
   ```



## 高斯贝叶斯



## 课后习题



## 参考

[朴素贝叶斯](https://www.cnblogs.com/huangyc/p/9734956.html)