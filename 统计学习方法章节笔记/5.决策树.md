# 第五章 决策树

## 本章概要

1. 决策树是⼀种基本的分类与回归方法。本章主要论用于分类的决策树。决策树可以看作是一个**if-then**规则的集合，也可以看作是定义在特征空间划分上的类的条件概率分布。
2. 决策树学习算法包括3部分：**特征选择、树的生成和树的剪枝。**
3. 决策树由结点和有向边组成。结点有两种类型：内部结点和叶结点。**内部结点表示⼀个特征或属性，叶结点表示⼀个类。**
4. **决策树学习本质上是从训练数据集中归纳出⼀组分类规则。**我们需要的是⼀个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。从另⼀个角度看，决策树学习是由训练数据集估计条件概率模型。
5. 决策树学习的损失函数通常是**正则化的极大似然函数**。决策树学习的策略是以损失函数为目标函数的最小化

因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用**启发式算法**，近似求解这⼀最优化问题。**这样得到的决策树是次最优的。**

## 决策数构建过程

开始，构建根结点，将所有训练数据都放在根结点。选择⼀个最优特征，按照这⼀特征将训练数据集分割成子集，使得各个子集有⼀个在当前条件下最好的分类。

如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有了明确的类。这就生成了⼀棵决策树。

**剪枝：**我们需要对生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。具体地，就是去掉过于细分的叶结点，使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。

**特征选择：**如果特征数量很多，也可以在决策树学习开始的时候，对特征进行选择，只留下对训练数据有足够分类能力的特征

## 特征选择

通常特征选择的准则是信息增益或信息增益比。（特征选择的基本方法有三种：**ID3的信息增益、C4.5的信息增益比、CART的基尼系数**）

例：<img src="C:/Users/Cakes/Desktop/images/%25E7%25BB%259F%25E8%25AE%25A1%25E5%25AD%25A6%25E4%25B9%25A0%25E6%2596%25B9%25E6%25B3%2595%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AC%2594%25E8%25AE%25B0/image-20211105134945714.png" alt="image-20211105134945714" style="zoom:90%;" />

<img src="C:/Users/Cakes/Desktop/images/%25E7%25BB%259F%25E8%25AE%25A1%25E5%25AD%25A6%25E4%25B9%25A0%25E6%2596%25B9%25E6%25B3%2595%25E5%25AD%25A6%25E4%25B9%25A0%25E7%25AC%2594%25E8%25AE%25B0/image-20211105135054852.png" alt="image-20211105135054852" style="zoom:75%;" />

如图5.3，分别由两个不同特征的根结点构成，问题是：究竟选择哪个特征更好些？

## 信息增益

在信息论与概率统计中，**熵是表示随机变量不确定性的度量**。设X是⼀个取有限个值的离散随机变量，其概率分布为$P(X=x_i)=p_i$,i=1,2,……,n，则随机变量X的熵定义为
$$
H(X)=-\sum_{i=1}^{n}p_i\log p_i
$$
通常，上式中的对数以2为底或以e为底，这时熵的单位分别称作比特（bit）或纳特（nat）。熵越大，随机变量的不确定性就越大。

设有随机变量(X,Y)，其联合概率分布为$P(X=x_i,Y=y_j)=p_{ij}$，$i=1,2,\cdots,n;j=1,2,\cdots ,m$，**条件熵**$H(Y|X)$表示在已知随机变量X的条件下随机变量Y的不确定性。$H(Y|X)$定义为X给定条件下Y的条件概率分布的熵对X的数学期望
$$
H(Y|X)=\sum_{i=1}^{n}p_iH(Y|X=x_i);p_i=P(X=x_i),i=1,2,\cdots,n
$$
当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵。

**信息增益**表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。