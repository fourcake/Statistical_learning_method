# 第五章 决策树

## 本章概要

1. 决策树是⼀种基本的分类与回归方法。本章主要论用于分类的决策树。决策树可以看作是一个**if-then**规则的集合，也可以看作是定义在特征空间划分上的类的条件概率分布。
2. 决策树学习算法包括3部分：**特征选择、树的生成和树的剪枝。**
3. 决策树由结点和有向边组成。结点有两种类型：内部结点和叶结点。**内部结点表示⼀个特征或属性，叶结点表示⼀个类。**
4. **决策树学习本质上是从训练数据集中归纳出⼀组分类规则。**我们需要的是⼀个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。从另⼀个角度看，决策树学习是由训练数据集估计条件概率模型。
5. 决策树学习的损失函数通常是**正则化的极大似然函数**。决策树学习的策略是以损失函数为目标函数的最小化

因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用**启发式算法**，近似求解这⼀最优化问题。**这样得到的决策树是次最优的。**

## 决策数构建过程

开始，构建根结点，将所有训练数据都放在根结点。选择⼀个最优特征，按照这⼀特征将训练数据集分割成子集，使得各个子集有⼀个在当前条件下最好的分类。

如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有了明确的类。这就生成了⼀棵决策树。

**剪枝：**我们需要对生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。具体地，就是去掉过于细分的叶结点，使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。

**特征选择：**如果特征数量很多，也可以在决策树学习开始的时候，对特征进行选择，只留下对训练数据有足够分类能力的特征

## 特征选择

通常特征选择的准则是信息增益或信息增益比。（特征选择的基本方法有三种：**ID3的信息增益、C4.5的信息增益比、CART的基尼系数**）

例：<img src="images/5.%E5%86%B3%E7%AD%96%E6%A0%91/image-20211108175550861.png" alt="image-20211108175550861" style="zoom:80%;" />

如图5.3，分别由两个不同特征的根结点构成，问题是：究竟选择哪个特征更好些？

### 信息增益

在信息论与概率统计中，**熵是表示随机变量不确定性的度量**。设X是⼀个取有限个值的离散随机变量，其概率分布为$P(X=x_i)=p_i$,i=1,2,……,n，则随机变量X的熵定义为
$$
H(X)=-\sum_{i=1}^{n}p_i\log p_i
$$
通常，上式中的对数以2为底或以e为底，这时熵的单位分别称作比特（bit）或纳特（nat）。熵越大，随机变量的不确定性就越大。

设有随机变量(X,Y)，其联合概率分布为$P(X=x_i,Y=y_j)=p_{ij}$，$i=1,2,\cdots,n;j=1,2,\cdots ,m$，**条件熵**$H(Y|X)$表示在已知随机变量X的条件下随机变量Y的不确定性。$H(Y|X)$定义为X给定条件下Y的条件概率分布的熵对X的数学期望
$$
H(Y|X)=\sum_{i=1}^{n}p_iH(Y|X=x_i);p_i=P(X=x_i),i=1,2,\cdots,n
$$
当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵。**上式信息增益**表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。

**定义**：特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即
$$
g(D,A)=H(D)-H(D|A)
$$
设训练数据集为D，|D|表示一共有多少样本，设有K个类$C_k$，$k=1,2,\cdots,K$，即特征的最终类别，设特征$A=\{a_1,a_2,\cdots,a_n\}$，根据A的特征值将D划分为N个子集$D_1,D_2,\cdots,D_n $。记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$

**信息增益算法：**

输入：训练数据集D，特征A

输出：特征A对训练数据集D的信息增益g(D,A)

1. 计算数据集D的经验熵H（D）
   $$
   H(D)=-\sum_{k=1}^{K}\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}
   $$

2. 计算特征A对数据集D的经验条件熵H(D|A)
   $$
   H(D|A)=\sum_i^n\frac{|D_i|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}
   $$

3. 计算信息增益$g(D,A)=H(D)-H(D|A)$

**例子计算过程见ID3算法**

**信息增益比**：特征A对训练数据集D的信息增益$g_R(D,A)$定义为其信息增益g(D,A)与训练数据集D的经验熵H(D)之比

在ID3算法中我们使用了**信息增益**来选择特征，在C4.5算法中，采用了**信息增益比**来选择特征，以减少信息增益容易选择特征值多的特征的问题。但是无论是ID3还是C4.5,都是基于[信息论的熵模型](https://www.cnblogs.com/huangyc/p/9734719.html)的，这里面会涉及大量的对数运算，比较耗时

CART分类树算法使用**基尼系数**来代替信息增益比，**基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的**。

具体的，在分类问题中，假设有K个类别，第k个类别的概率为pk, 则基尼系数的表达式为：
$$
Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2
$$

## 决策树的生成

###  ID3算法（信息增益）

ID3算法的核心是在决策树各个节点上应用信息增益选择特征，递归的构建决策树。

输⼊：训练数据集D，特征集A，阈值$\varepsilon$
输出：决策树T

1. 若D中所有实例属于同⼀类C_k,则T为单结点树，并将类Ck作为该结点的类标记，返回T
2. 若A＝Ø，则T为单结点树，并将D中实例数最多的类Ck 作为该结点的类标记，返回T
3. 否则，按算法计算A中各特征对D的信息增益，选择信息增益最大的特征Ag 
4. 如果Ag的信息增益小于阈值 ，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T
5. 否则，对Ag的每⼀可能值ai ，依Ag ＝ai将D分割为若干非空子集Di，将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T
6. 递归调用1-5

### C4.5生成算法（信息增益比）

整体一致，只是根据信息增益比来选择特征

## 决策树的减枝

剪枝从已生成的树上裁掉⼀些子树或叶结点，并将其根结点或父结点作为新的叶结点

**损失函数**
$$
C_a(T)=\sum_{t=1}^{|T|}N_tH_t(T)+a|T|=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{tk}\log\frac{N_{tk}}{N_t}+a|T|=C(T)+a|T|
$$
剪枝，就是当a确定时，选择损失函数最小的模型，即损失函数最小的子树

输⼊：生成算法产生的整个树T，参数a；
输出：修剪后的子树Ta 。

1. 计算每个结点的经验熵

2. 递归地从树的叶结点向上回缩

   设⼀组叶结点回缩到其父结点之前与之后的整体树分别为TB 与TA ，其对应的损失函数值分别是Ca (TB )与Ca (TA )，如果$C_a(T_A)≤C_a(T_B)$，则进行剪枝，即将父结点变为新的叶结点

3. 返回2，直至不能继续为止，得到损失函数最小的子树Ta 

## CART(分类回归树----基尼系数+平方误差最小化)

CART是在给定输⼊随机变量X条件下输出随机变量Y的条件概率分布的学习方法。CART**假设**决策树是**⼆叉树**，内部结点特征的取值
为是和否，左分支是取值为是的分支，右分支是取值为否的分支

CART算法由以下两步组成：
（1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大
（2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

输入：训练集D，基尼系数的阈值ε1，样本个数阈值ε2。

输出：决策树T。

1. 对于当前节点的数据集为DD，如果样本个数小于阈值ε2ε2或者没有特征，则返回决策子树，当前节点停止递归。
2. 计算样本集DD的基尼系数，如果基尼系数小于阈值ε1ε1，则返回决策树子树，当前节点停止递归。
3. 计算当前节点现有的各个特征的各个特征值对数据集DD的基尼系数。
4. 在计算出来的各个特征的各个特征值对数据集DD的基尼系数中，选择基尼系数最小的特征AA和对应的特征值aa。根据这个最优特征和最优特征值，把数据集划分成两部分D1D1和D2D2，同时建立当前节点的左右节点，做节点的数据集DD为D1D1，右节点的数据集DD为D2D2。
5. 对左右的子节点递归的调用1-4步，生成决策树

## 算法实现

给定产生数据，新数据也得按如下形式排列

```python
def createDataSet():
    dataSet = [[1, 1, 'yes'],
               [1, 1, 'yes'],
               [1, 0, 'no'],
               [0, 1, 'no'],
               [0, 1, 'no']]
    labels = ['no surfacing','flippers']
    return dataSet, labels
```

1. 计算给定数据集的熵

   ```python
   #计算给定数据集的熵H(X)
   def calcShannonEnt(dataSet):
       numEntries = len(dataSet)
       labelCounts = {}#创建字典
       for featVec in dataSet: 
           currentLabel = featVec[-1]#键值是每个特征向量最后一列的值，即对应标签
           if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0
           labelCounts[currentLabel] += 1#如果字典中没有这个键值，则创建并加一
       shannonEnt = 0.0
       for key in labelCounts:
           prob = float(labelCounts[key])/numEntries
           shannonEnt -= prob * log(prob,2) #log base 2
       return shannonEnt#H(X)
   ```

2.  划分数据集

   ```python
   #输入为待划分数据集，划分数据集的特征，根据该特征的取值划分剩余数据集
   def splitDataSet(dataSet, axis, value):
       retDataSet = []
       for featVec in dataSet:
           if featVec[axis] == value:
               reducedFeatVec = featVec[:axis]#为空，用来存放被划分的数据
               reducedFeatVec.extend(featVec[axis+1:])#因为从第零个属性开始的，所以每次把之后的属性保存就好
               retDataSet.append(reducedFeatVec)
       return retDataSet
   ```

3. 比较g（D|A）

   ```python
   #比较G(D|A),得到信息增益最好的一种属性索引
   def chooseBestFeatureToSplit(dataSet):
       numFeatures = len(dataSet[0]) - 1    #最后一列是labels,numFeatures为当前数据集有多少种属性
       baseEntropy = calcShannonEnt(dataSet)#H(X)
       bestInfoGain = 0.0   #g(D,A)
       bestFeature = -1#表第几列最好
       for i in range(numFeatures):        #遍历所有属性种类，即（x1，x2，……，xn，y），有n种属性
           featList = [example[i] for example in dataSet]#将dataSet中的数据先按行依次放入example中，然后取得每个example中的example[i]元素，放入列表featList中
           uniqueVals = set(featList)       #获得该属性有多少种取值，分别是多少
           newEntropy = 0.0
           for value in uniqueVals:#H(D|A)
               subDataSet = splitDataSet(dataSet, i, value)
               prob = len(subDataSet)/float(len(dataSet))
               newEntropy += prob * calcShannonEnt(subDataSet)     
           infoGain = baseEntropy - newEntropy#g(D,A)=H(D)-H(D|A)
           if (infoGain > bestInfoGain):       #每个属性计算完比较一下，取最优的
               bestInfoGain = infoGain        
               bestFeature = i
       return bestFeature                      #returns an integer
   ```

4. 多数表决

   ```python
   #多数表决
   def majorityCnt(classList):
       classCount={}
       for vote in classList:
           if vote not in classCount.keys(): classCount[vote] = 0
           classCount[vote] += 1
       sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)
       return sortedClassCount[0][0]
   ```

5. 构建决策树

   ```python
   #递归构建决策树
   def createTree(dataSet,labels):
       classList = [example[-1] for example in dataSet]#将dataSet中的数据先按行依次放入example中，然后取得每个example中的最后一列元素，放入列表classList中
       if classList.count(classList[0]) == len(classList): 
           return classList[0]#如果所有的类标签都相同则返回
       if len(dataSet[0]) == 1: #使用完所有属性仍不能被归类，则归到多的那一类
           return majorityCnt(classList)
       bestFeat = chooseBestFeatureToSplit(dataSet)#得到当前用于分类的最好的属性
       bestFeatLabel = labels[bestFeat]
       myTree = {bestFeatLabel:{}}
       del(labels[bestFeat])#使用过的属性删去不再使用
       featValues = [example[bestFeat] for example in dataSet]
       uniqueVals = set(featValues)
       #遍历当前选择属性的所有值，在每个数据集划分上递归调用createTree，得到返回值插入到字典变量myTree中
       for value in uniqueVals:
           subLabels = labels[:]       #copy all of labels, so trees don't mess up existing labels
           myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)
       return myTree  
   ```

6. 使用决策树

   ```python
   #使用决策树
   def classify(inputTree,featLabels,testVec):
       firstStr = list(inputTree.keys())[0]
       secondDict = inputTree[firstStr]
       featIndex = featLabels.index(firstStr)
       for key in secondDict.keys():
           if testVec[featIndex]==key:
               if type(secondDict[key])  == dict:
                   classLabel=classify(secondDict[key],featLabels,testVec)
               else: classLabel = secondDict[key]
       return classLabel
   
   classify(myTree,myLabels,[1,0])
   ```

   