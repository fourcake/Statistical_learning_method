# 第八章 提升方法

在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。

## 提升方法AdaBoost算法

**基本思路：**对于⼀个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何⼀个专家单独的判断好

在学习中，如果已经发现了“弱学习算法”，那么能否将它提升（boost）为“强学习算法”，最具代表性的提升算法是AdaBoost。提升方法就是从弱学习算法出发，反复学习，得到⼀系列弱分类器（又称为基本分类器），然后组合这些弱分类器，构成⼀个强分类器。有两个问题：

1. 在每⼀轮如何改变训练数据的权值或概率分布

   提高那些被前⼀轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值

2. 如何将弱分类器组合成一个强分类器

   AdaBoost采取加权多数表决的方法

## AdaBoost算法

输入：训练数据集$T=\{(x_1,y_1),(x_2,y_2),……,(x_N,y_N)\};y_i∈\{-1,+1\};弱学习算法$

输出：最终分类器

1. 初始化训练数据的权值分布
   $$
   D_1=(w_{11},\cdots,w_{1i},\cdots,w_{1N}),w_{1i}=\frac{1}{N},i=1,2,\cdots,N
   $$

2. 对M=1，2，……，m

   1. 使用具有权值分布Dm的训练数据集学习，得到基本分类器

      <img src="images/8.%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95AdaBoost/image-20211117172210298.png" alt="image-20211117172210298" style="zoom:80%;" />

   2. 计算Gm(x)在训练数据集上的分类误差率
      $$
      e_m=P(G_m(x_i)≠y_i)=\sum_{i=1}^{N}w_{mi}I(G_m(x_i)≠y_i)
      $$

   3. 计算Gm(x)的系数，用到的是自然对数
      $$
      a_m=\frac{1}{2}log\frac{1-e_m}{e_m}
      $$

   4. 更新训练数据集的权值分布
      $$
      D_{m+1}=(w_{m+1,1},\cdots,w_{m+1,i},\cdots,w_{m+1,N})
      $$
      其中
      $$
      w_{m+1,i}=\frac{w_{mi}}{Z_m}exp(-a_my_iG_m(x_i)),i=1,2,\cdots,N
      $$
      这里Zm是规范化因子，它使Dm+1成为一个概率分布。
      $$
      Z_m=\sum_{i=1}^{N}w_{mi}exp(-a_my_iG_m(x_i))
      $$

3. 构建基本分类器的线性组合
   $$
   f(x)=\sum_{m=1}^{M}a_mG_m(x)
   $$
   得到最终分类器
   $$
   G(x)=sign(f(x))=sign(\sum_{m=1}^{M}a_mG_m(x))
   $$

## AdaBoost例子

给定下表训练数据。假设弱分类器由x<v或x>v产生，其阈值v使该分类器在训练数据集上分类误差率最低。试用AdaBoost算法学习⼀个强分类器。

<img src="images/8.%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95AdaBoost/image-20211117172229736.png" alt="image-20211117172229736" style="zoom:80%;" />

![image-20211117172256873](images/8.%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95AdaBoost/image-20211117172256873.png)

## AdaBoost算法解释

可以认为AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法时的⼆类分类学习方法。

### 前向分步算法

考虑加法模型
$$
f(x)=\sum_{m=1}^{M}β_mb(x;γ_m)
$$
其中，b(x,γ)是基函数，γm是基函数的参数，β是系数。在给定训练数据及损失函数L(Y,f(X))的条件下，学习加法模型f(x)成
为经验风险极小化，即损失函数极小化问题：

<img src="images/8.%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95AdaBoost/image-20211117172312980.png" alt="image-20211117172312980" style="zoom:80%;" />

前向分步算法求解这⼀优化问题的想法是：因为学习的是加法模型，如果能够从前向后，每⼀步只学习⼀个基函数及其系数，逐步逼近优化目标函数式（8.14），那么就可以简化优化的复杂度

AdaBoost算法是前向分歩加法算法的特例，这时，模型是由基本分类器组成的加法模型，损失函数是指数函数

## 提升树

提升树是以分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一

提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法。以决策树为基函数的提升方法称为提升树

### 提升树算法

对于⼆类分类问题，提升树算法只需将AdaBoost算法中的基本分类器限制为二类分类树即可，这里我们讨论回归问题的提升树算法

输入：训练数据集$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$

输出：提升树$f_M(x)$

1. 初始化$f_0(x)=0$

2. 对m=1,2……，M

   1. 按下式计算残差
      $$
      r_{mi}=y_i-f_{m-1}(x_i),i=1,2,\cdots,N
      $$

   2. 拟合残差rmi学习一个回归树，得到$T(x;θ_m)$

   3. 更新$f_m(x)=f_{m-1}(x)+T(x;θ_m)$

3. 得到回归问题提升树
   $$
   f_M(x)=\sum_{m=1}^{M}T(x;θ_m)
   $$

**例题8.2**：已知如下表的训练数据，x的取值范围为区间[0.5,10.5]，y的取值范围为区间[5.0,10.0]，学习这个回归问题的提升树模型，考虑只用树桩作为基函数。

![image-20211117172329399](images/8.%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95AdaBoost/image-20211117172329399.png)

<img src="images/8.%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95AdaBoost/image-20211117172352525.png" alt="image-20211117172352525" style="zoom:90%;" />

<img src="images/8.%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95AdaBoost/image-20211117172436092.png" alt="image-20211117172436092" style="zoom:90%;" />

## 代码实现

1. 载入数据

   ```python
   def loadSimpData():
       datMat = matrix([[ 1. ,  2.1],
           [ 2. ,  1.1],
           [ 1.3,  1. ],
           [ 1. ,  1. ],
           [ 2. ,  1. ]])
       classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]
       return datMat,classLabels
   #更具一般性的载入数据
   
   def loadDataSet(fileName):      
       numFeat = len(open(fileName).readline().split('\t')) 
       dataMat = []; labelMat = []
       fr = open(fileName)
       for line in fr.readlines():
           lineArr =[]
           curLine = line.strip().split('\t')
           for i in range(numFeat-1):
               lineArr.append(float(curLine[i]))
           dataMat.append(lineArr)
           labelMat.append(float(curLine[-1]))
       return dataMat,labelMat
   ```

2. 构建单层决策树

   ```python
   #判断数据的分类，在阈值两侧分别为1，-1，用数组过滤赋值
   def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):#just classify the data
       retArray = ones((shape(dataMatrix)[0],1))
       if threshIneq == 'lt':
           retArray[dataMatrix[:,dimen] <= threshVal] = -1.0
       else:
           retArray[dataMatrix[:,dimen] > threshVal] = -1.0
       return retArray
   #建立单层决策树
   def buildStump(dataArr,classLabels,D):
       dataMatrix = mat(dataArr)
       labelMat = mat(classLabels).T
       m,n = shape(dataMatrix)
       numSteps = 10.0
       bestStump = {}#字典，用来存储决策树
       bestClasEst = mat(zeros((m,1)))
       minError = inf #init error sum, to +infinity
       for i in range(n):#遍历每一维特征,最终找出一个最佳维度的特征作为单层决策树
           rangeMin = dataMatrix[:,i].min()
           rangeMax = dataMatrix[:,i].max()
           stepSize = (rangeMax-rangeMin)/numSteps
           for j in range(-1,int(numSteps)+1):#loop over all range in current dimension
               for inequal in ['lt', 'gt']: #lt为满足小于等于，gt为满足大于等于
                   threshVal = (rangeMin + float(j) * stepSize)#阈值
                   predictedVals = stumpClassify(dataMatrix,i,threshVal,inequal)#根据阈值和不等式，计算预测的分类
                   #print(threshVal)
                   #print(predictedVals)
                   #计算分类误差率em即weightedError
                   errArr = mat(ones((m,1)))
                   errArr[predictedVals == labelMat] = 0
                   #print(D.T)
                   #print(errArr)
                   weightedError = D.T*errArr  #计算误差
                   #print( "split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f" % (i, threshVal, inequal, weightedError))
                   if weightedError < minError:
                       minError = weightedError
                       bestClasEst = predictedVals.copy()
                       bestStump['dim'] = i
                       bestStump['thresh'] = threshVal
                       bestStump['ineq'] = inequal
       return bestStump,minError,bestClasEst
   ```

3. 完整AdaBoost

   ```python
   #完整AdaBoost                           
   def adaBoostTrainDS(dataArr,classLabels,numIt=40):
       weakClassArr = []
       m = shape(dataArr)[0]
       D = mat(ones((m,1))/m)   #D包含每个数据点的权重，因为是概率分布所以和为1
       aggClassEst = mat(zeros((m,1)))
       for i in range(numIt):
           bestStump,error,classEst = buildStump(dataArr,classLabels,D)#build Stump
          # print("D:",D.T)
           alpha = float(0.5*log((1.0-error)/max(error,1e-16)))#计算alpha。即G1（x）的系数
           bestStump['alpha'] = alpha  
           weakClassArr.append(bestStump)                  #存储单层分类器
          # print("classEst: ",classEst.T)#根据单层决策树预测分类结果
           #为下一次迭代计算D
           expon = multiply(-1*alpha*mat(classLabels).T,classEst) 
          # print("expon: ",expon)公式8.3-8.5
           D = multiply(D,exp(expon))                            
           D = D/D.sum()
           #错位率累加计算，若是为0则停止循环
           aggClassEst += alpha*classEst
          # print ("aggClassEst: ",aggClassEst.T)
           aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,1)))
           errorRate = aggErrors.sum()/m
           #print("total error: ",errorRate)
           if errorRate == 0.0: break
       return weakClassArr,aggClassEst
   ```

4. 测试分类器

   ```python
   #输入为待分样例datToClass和弱分类器组合classifierArr
   def adaClassify(datToClass,classifierArr):
       dataMatrix = mat(datToClass)#转为numpy矩阵
       m = shape(dataMatrix)[0]
       aggClassEst = mat(zeros((m,1)))
       for i in range(len(classifierArr)):
           classEst = stumpClassify(dataMatrix,classifierArr[i]['dim'],\
                                    classifierArr[i]['thresh'],\
                                    classifierArr[i]['ineq'])#call stump classify
           aggClassEst += classifierArr[i]['alpha']*classEst
           print(aggClassEst)
       return sign(aggClassEst)
   adaClassify([0,0],classFiers)
   ```

